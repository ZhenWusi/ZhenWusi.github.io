<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>RLHF 的三阶段流程</title>
      <link href="/2026/02/23/RLHF%20%E7%9A%84%E4%B8%89%E9%98%B6%E6%AE%B5%E6%B5%81%E7%A8%8B/"/>
      <url>/2026/02/23/RLHF%20%E7%9A%84%E4%B8%89%E9%98%B6%E6%AE%B5%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><script type="math/tex; mode=display">\begin{aligned}&\textbf{阶段概览} \\&\text{RLHF（Reinforcement Learning from Human Feedback）包含三大流程：} \\&\text{监督微调（SFT）} \rightarrow \text{偏好采样与奖励建模} \rightarrow \text{强化学习优化（RL）。} \\&\text{直观理解：先学“怎么说话”，再学“什么话更好”，最后推动模型输出更优回答。}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}&\textbf{阶段一：监督微调（SFT）} \\&\text{起点是对一个预训练语言模型执行监督微调，以人工高质量数据构建 } \pi_{SFT}。 \\&\text{数据形式：}(x, y) \text{，其中 } x \text{ 为 prompt，} y \text{ 为人工答案。} \\&\text{目标：掌握基本对话与任务能力，即“怎么说话”。}\end{aligned}</script><script type="math/tex; mode=display">\max_{\theta} \sum_{(x,y) \in D} \log \pi_\theta(y|x)</script><script type="math/tex; mode=display">\begin{aligned}&\textbf{阶段二：奖励模型（Reward Modeling）} \\&\text{用 } \pi_{SFT} \text{ 对同一 prompt } x \text{ 采样两个回答 } y_1, y_2。 \\&\text{人类标注给出偏好： } y_w \succ y_l \mid x。 \\&y_w: \text{赢家（preferred）}, \quad y_l: \text{输家（rejected）。} \\&\text{假设存在潜在真实奖励 } r^*(x,y)，\text{偏好来自该奖励差异。}\end{aligned}</script><script type="math/tex; mode=display">y_1, y_2 \sim \pi_{SFT}(y|x)</script><script type="math/tex; mode=display">y_w \succ y_l \mid x</script><script type="math/tex; mode=display">p^*(y_1 > y_2 | x) = \frac{\exp(r^*(x, y_1))}{\exp(r^*(x, y_1)) + \exp(r^*(x, y_2))} \tag{1}</script><script type="math/tex; mode=display">D = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N</script><script type="math/tex; mode=display">\mathcal{L}_\text{R}(r_\phi, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma (r_\phi(x, y_w) - r_\phi(x, y_l)) \right] \tag{2}</script><script type="math/tex; mode=display">\begin{aligned}&\textbf{Bradley-Terry（BT）模型说明} \\&\text{每个对象 } i \text{ 具有潜在实力 } \lambda_i。 \\&\text{比较 } i \text{ 与 } j \text{ 时，战胜概率：}\end{aligned}</script><script type="math/tex; mode=display">P(i \succ j) = \frac{\lambda_i}{\lambda_i + \lambda_j}</script><script type="math/tex; mode=display">\begin{aligned}&\text{在 RLHF 场景中：} r(x,y) \text{ 视作实力值，偏好数据即比较结果。} \\&\text{用概率模型刻画人类噪声，而非硬性“分数更大者必胜”。}\end{aligned}</script><script type="math/tex; mode=display">y_1 \succ y_2 \iff r^*(x,y_1) > r^*(x,y_2)</script><script type="math/tex; mode=display">p^*(y_1 \succ y_2 \mid x) = \frac{1}{1 + e^{-(r^*(x,y_1) - r^*(x,y_2))}}</script><script type="math/tex; mode=display">\sigma(z) = \frac{1}{1 + e^{-z}}</script><script type="math/tex; mode=display">p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x,y_1) - r^*(x,y_2))</script><script type="math/tex; mode=display">\log p_\theta(y_w \succ y_l \mid x) = \log \sigma(r_\theta(x,y_w) - r_\theta(x,y_l))</script><script type="math/tex; mode=display">\max_\theta \sum_{i=1}^N \log \sigma(r_\theta(x^{(i)}, y_w^{(i)}) - r_\theta(x^{(i)}, y_l^{(i)}))</script><script type="math/tex; mode=display">\begin{aligned}&\text{对数把连乘变为连加，不改变最优解（单调递增）。} \\&\text{因此常等价为最小化负对数似然：}\end{aligned}</script><script type="math/tex; mode=display">L_R = -\mathbb{E}_{(x,y_w,y_l) \sim D} [\log \sigma(r_\theta(x,y_w) - r_\theta(x,y_l))]</script><script type="math/tex; mode=display">\begin{aligned}&\text{有限数据集 } D = \{z_1,\ldots,z_N\} \text{ 上，期望等于平均值：}\end{aligned}</script><script type="math/tex; mode=display">\mathbb{E}_{z \sim D} [f(z)] = \frac{1}{N} \sum_{i=1}^N f(z_i)</script><script type="math/tex; mode=display">\begin{aligned}&\text{设 } f(x,y_w,y_l) = \log \sigma(r_\theta(x,y_w) - r_\theta(x,y_l))，\text{则：}\end{aligned}</script><script type="math/tex; mode=display">\mathbb{E}_{(x,y_w,y_l) \sim D} [f] = \frac{1}{N} \sum_{i=1}^N \log \sigma(r_\theta(x^{(i)}, y_w^{(i)}) - r_\theta(x^{(i)}, y_l^{(i)}))</script><script type="math/tex; mode=display">\begin{aligned}&\text{乘以正的常数不改变最优解：}\end{aligned}</script><script type="math/tex; mode=display">\arg\max_\theta \sum_{i=1}^N f_i = \arg\max_\theta \frac{1}{N} \sum_{i=1}^N f_i</script><script type="math/tex; mode=display">\begin{aligned}&\max_\theta \mathbb{E}_D[\log \sigma(\cdots)] = \min_\theta -\mathbb{E}_D[\log \sigma(\cdots)] \\&\text{因此记作 } L_R = -\mathbb{E}_D[\log \sigma(\cdots)]，\text{配合梯度下降习惯“最小化损失”。}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}&\textbf{阶段三：RL 微调} \\&\text{策略在奖励模型指导下继续优化，同时受 KL 约束保持与 } \pi_{SFT} \text{ 接近。}\end{aligned}</script><script type="math/tex; mode=display">\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y|x)} \left[ r_\phi(x, y) - \beta D_{\text{KL}} \left[ \pi_\theta(y|x) \middle\| \pi_{\text{ref}}(y|x) \right] \right] \tag{3}</script><script type="math/tex; mode=display">\begin{aligned}&\text{初始条件：} \pi_\theta = \pi_{ref} = \pi_{SFT}。 \\&\text{目标：保持与参考策略接近的同时最大化奖励。} \\&\text{KL 惩罚确保模型不过度偏离原始能力。}\end{aligned}</script><script type="math/tex; mode=display">D_{\text{KL}}[P \| Q] = \sum_x P(x) \log \frac{P(x)}{Q(x)}</script><script type="math/tex; mode=display">D_{\text{KL}}[\pi_\theta(y|x) \| \pi_{SFT}(y|x)] = \mathbb{E}_{y \sim \pi_\theta} \left[ \log \frac{\pi_\theta(y|x)}{\pi_{SFT}(y|x)} \right]</script><script type="math/tex; mode=display">\begin{aligned}&\text{若 } \pi_\theta = \pi_{SFT} \Rightarrow KL = 0；\text{偏离越大，KL 越大，故需惩罚。}\end{aligned}</script><script type="math/tex; mode=display">D_{\text{KL}}[P \| Q] = \sum_i P(i) \log \frac{P(i)}{Q(i)}</script><script type="math/tex; mode=display">\mathbb{E}_{Y \sim P}[f(Y)] = \sum_{y \in \mathcal{Y}} P(y) f(y)</script><script type="math/tex; mode=display">\begin{aligned}&P = \pi_\theta(y|x), \quad Q = \pi_{SFT}(y|x), \quad \mathcal{Y} = \text{所有可能回答序列}。 \\&\text{代入得：}\end{aligned}</script><script type="math/tex; mode=display">D_{\text{KL}}[\pi_\theta \| \pi_{SFT}] = \sum_y \pi_\theta(y|x) \log \frac{\pi_\theta(y|x)}{\pi_{SFT}(y|x)}</script><script type="math/tex; mode=display">D_{\text{KL}}[\pi_\theta \| \pi_{SFT}] = \mathbb{E}_{y \sim \pi_\theta(y|x)} \left[ \log \frac{\pi_\theta(y|x)}{\pi_{SFT}(y|x)} \right]</script><script type="math/tex; mode=display">\begin{aligned}&\text{写成期望的原因：} \\&1. \text{语言模型输出空间巨大，无法直接求和。} \\&2. \text{期望形式便于用蒙特卡洛采样近似。} \\&3. \text{与原目标 } \mathbb{E}_{y \sim \pi_\theta}[r(x,y)] \text{ 形式一致，便于合并。}\end{aligned}</script><script type="math/tex; mode=display">\max_{\pi_\theta} \mathbb{E}_{y \sim \pi_\theta} \left[ r(x,y) - \beta \log \frac{\pi_\theta(y|x)}{\pi_{SFT}(y|x)} \right]</script><script type="math/tex; mode=display">\begin{aligned}&\text{离散生成导致目标不可直接对 } \theta \text{ 求导；需借助策略梯度（REINFORCE/PPO）。}\end{aligned}</script><script type="math/tex; mode=display">D_{\text{KL}}(\pi_\theta \| \pi_{ref}) = \mathbb{E}_{y \sim \pi_\theta} \left[ \log \pi_\theta(y|x) - \log \pi_{ref}(y|x) \right]</script><script type="math/tex; mode=display">\mathbb{E}_{y \sim \pi_\theta} \left[ r_\phi(x, y) - \beta \left( \log \pi_\theta(y|x) - \log \pi_{ref}(y|x) \right) \right]</script><script type="math/tex; mode=display">r(x, y) = r_\phi(x, y) - \beta \left( \log \pi_\theta(y|x) - \log \pi_{ref}(y|x) \right)</script><script type="math/tex; mode=display">r(x, y) = r_\phi(x, y) - \left( \log \pi_\theta(y|x) - \log \pi_{ref}(y|x) \right)</script><script type="math/tex; mode=display">\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y|x)} \left[ r(x, y) \right]</script><script type="math/tex; mode=display">\begin{aligned}&\text{上式中的 } r(x,y) \text{ 已包含 KL 惩罚，因此目标转化为无约束奖励最大化。}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}&\textbf{直接偏好优化（DPO）提要} \\&\text{DPO 直接对偏好数据开展优化，省略显式奖励模型，} \\&\text{其核心同样可被写成纯粹的 LaTeX 目标函数与对偶形式。}\end{aligned}</script>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>力扣 88：合并两个有序数组</title>
      <link href="/2026/02/23/%E5%8A%9B%E6%89%A3/"/>
      <url>/2026/02/23/%E5%8A%9B%E6%89%A3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定两个按非递减顺序排列的整数数组 <code>nums1</code> 和 <code>nums2</code>，分别有长度 <code>m</code>、<code>n</code>。请将 <code>nums2</code> <strong>原地</strong> 合并到 <code>nums1</code>，使结果仍保持非递减顺序。<code>nums1</code> 的总长度为 <code>m + n</code>，末尾的 <code>n</code> 个位置初始化为 <code>0</code> 作为缓冲区。</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>与其先拼接再整体排序，不如利用“两数组已排序”这一特性，从尾部开始归并：</p><ol><li>设 <code>p1 = m - 1</code>、<code>p2 = n - 1</code> 分别指向两个数组的最后一个有效元素，<code>tail = m + n - 1</code> 指向合并后数组的尾部；</li><li>每轮比较 <code>nums1[p1]</code> 和 <code>nums2[p2]</code>，把较大的放入 <code>nums1[tail]</code>，然后移动对应指针以及 <code>tail</code>；</li><li>若 <code>nums2</code> 还有剩余元素（<code>p2 &gt;= 0</code>），继续写入；如果只剩 <code>nums1</code> 元素，无需处理，因为它们已经在正确位置。</li></ol><p>双指针从后往前填充可以避免移动大量元素，时间复杂度为 $\mathcal{O}(m+n)$，空间复杂度为 $\mathcal{O}(1)$。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">merge</span>(<span class="params">self, nums1: <span class="type">List</span>[<span class="built_in">int</span>], m: <span class="built_in">int</span>, nums2: <span class="type">List</span>[<span class="built_in">int</span>], n: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        p1 = m - <span class="number">1</span></span><br><span class="line">        p2 = n - <span class="number">1</span></span><br><span class="line">        tail = m + n - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> p2 &gt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> p1 &gt;= <span class="number">0</span> <span class="keyword">and</span> nums1[p1] &gt; nums2[p2]:</span><br><span class="line">                nums1[tail] = nums1[p1]</span><br><span class="line">                p1 -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                nums1[tail] = nums2[p2]</span><br><span class="line">                p2 -= <span class="number">1</span></span><br><span class="line">            tail -= <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><strong>时间复杂度</strong>：$\mathcal{O}(m+n)$，每个元素只移动一次；</li><li><strong>空间复杂度</strong>：$\mathcal{O}(1)$，原地合并未使用额外数组。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 力扣 </tag>
            
            <tag> 数组 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
