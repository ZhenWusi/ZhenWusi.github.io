<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Scaffold-GS</title>
      <link href="/2024/02/03/scaffold-gs/"/>
      <url>/2024/02/03/scaffold-gs/</url>
      
        <content type="html"><![CDATA[<h1 id="Scaffold-GS-Structured-3D-Gaussians-for-View-Adaptive-Rendering-用于视图自适应渲染的结构化3D高斯函数"><a href="#Scaffold-GS-Structured-3D-Gaussians-for-View-Adaptive-Rendering-用于视图自适应渲染的结构化3D高斯函数" class="headerlink" title="Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering(用于视图自适应渲染的结构化3D高斯函数)"></a>Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering(用于视图自适应渲染的结构化3D高斯函数)</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><font size="2">Neural rendering methods have significantly advanced photo-realistic 3D scene rendering in various academic and industrial applications. The recent 3D Gaussian Splatting method has achieved state-of-the-art rendering quality and speed by combining the benefits of both primitive-based representations and volumetric representations. However, it often leads to heavily redundant Gaussians that try to fit every training view, neglecting the underlying scene geometry. Consequently, the resulting model becomes less robust to significant view changes, texture-less areas, and lighting effects.We introduce Scaffold-GS, which uses anchor points to distribute local 3D Gaussians and predicts their attributes on-the-fly based on viewing direction and distance within the view frustum. Anchor growing and pruning strategies are developed based on the importance of neural Gaussians to reliably improve the scene coverage. We show that our method effectively reduces redundant Gaussians while delivering high-quality rendering. We also demonstrate an enhanced capability to accommodate scenes with varying levels-of-detail and view-dependent observations, without sacrificing rendering speed.</font></p><p>神经渲染方法在各种学术和工业应用中显着提高了照片级真实感 3D 场景渲染的性能。 最近的 3D 高斯喷射方法结合了基于基元(primitive,基本单元)表示和体积表示的优势，实现了最先进的渲染质量和速度。 然而，它通常会导致严重冗余的高斯模型，试图适应每个训练视图，而忽略了底层的场景几何形状。 因此，生成的模型对于显着的视图变化、无纹理区域和照明效果变得不太稳健。 我们引入了 Scaffold-GS，<font color="Red"><strong>它使用锚点来分布局部3D 高斯，并根据视锥体内的观察方向和距离动态预测它们的属性</strong></font>。 锚点生长和修剪策略是基于神经高斯模型对于可靠地提高场景覆盖范围的重要性而开发的。 我们表明，我们的方法有效地减少了冗余高斯，同时提供高质量的渲染。 <font color="Red"><strong>我们还展示了一种增强的能力，可以适应具有不同细节级别和依赖于视图的观察的场景，而无需牺牲渲染速度。</strong></font></p><hr><p>使用一组在双层层次结构中结构的3D高斯函数来表示场景。锚定在初始点的稀疏网格上，从每个锚点衍生出一组适度的神经高斯函数，以动态适应不同的视角和距离。该方法在具有挑战性观察视图的场景中表现出色。例如，透明度、镜面反射、反射、无纹理区域和精细比例细节。<br><img src="/2024/02/03/scaffold-gs/Scaffold-GS%5Cteaser_big.png" alt="复杂视图与3dgs对比"></p><hr><h2 id="本文贡献如下："><a href="#本文贡献如下：" class="headerlink" title="本文贡献如下："></a>本文贡献如下：</h2><p>1)利用场景结构，我们从<font color="Red"><strong>稀疏体素网格中初始化锚点来指导局部三维高斯分布</strong></font>，形成分层和区域感知的场景表示;</p><p>2)在视锥内，我们<font color="Red"><strong>实时预测每个锚点的神经高斯分布，以适应不同的观看方向和距离，</strong></font>从而产生更鲁棒的新视图合成;</p><p>3)我们开发了一个更可靠的<font color="Red"><strong>锚点生长和修剪策略，利用预测的神经高斯来更好地覆盖场景。</strong></font></p><h2 id="原理："><a href="#原理：" class="headerlink" title="原理："></a>原理：</h2><p>我们首先从 SFM(运动恢复结构) 衍生的点形成一个稀疏体素(voxel)网格。每个可见体素的中心都放置有一个可学习尺度(learnable offset)的锚点(anchor)，大致雕刻了场景的占用情况。（b） 视锥内，从每个可见的带有偏移量𝑂_𝑘锚点生成 k 个神经高斯。它们的属性，即不透明度(opacity)、颜色、比例和四元数(quaternion)是使用MLP从锚点特征、相对相机-锚点的观看方向和距离进行解码,使用 Fα、Fc、Fs、Fq表示。 （c）神经高斯模型属性的预测是即时的，意味着只有在视锥体内可见的锚点才会被激活以生成神经高斯模型。为了使栅格化更加高效，我们仅保留透明度值大于预定义阈值 τα 的神经高斯模型。这大大降低了计算负载，并帮助我们的方法保持与原始3D-GS相当的高渲染速度。通过重建L_1、结构相似性L_𝑆𝑆𝐼𝑀和体积正则化L_𝑣𝑜𝑙来监督渲染图像。<br><img src="/2024/02/03/scaffold-gs/Scaffold-GS%5Cpipeline.png" alt="原理图"></p><h3 id="描点细化"><a href="#描点细化" class="headerlink" title="描点细化"></a>描点细化</h3><p>增长操作：由于神经高斯模型与它们的锚点紧密关联，而<font color="Red"><strong>这些锚点是从SFM点初始化的，它们的建模能力受限于局部区域，特别是在无纹理和少观察的区域。</strong></font>因此，我们提出了一种基于误差的锚点增长策略，即在神经高斯模型发现显著的地方增加新的锚点。为了确定显著的区域，我们首先通过构建大小为ϵg的体素来空间量化神经高斯模型。对于每个体素，我们计算包含的神经高斯模型在N个训练迭代中的平均梯度，表示为∇g。然后，对于∇g &gt; τg的体素，我们认为其是显著的，其中τg是预定义的阈值；<font color="Red"><strong>如果该体素中没有已建立的锚点，则在该体素的中心部署一个新的锚点。</strong></font>图3说明了这个增长操作。在实践中，我们将空间量化为多分辨率的体素网格，以允许在不同的粒度级别添加新的锚点，其中ϵ(m)g = ϵg/4m−1，τ(m)g = τg ∗ 2m−1，（其中m表示量化的级别）。为了进一步调控新锚点的添加，我们对这些候选进行随机消除。这种谨慎的方法有效地抑制了锚点的快速扩展。<br>修剪操作：为了消除无关的锚点，我们累积了与它们关联的神经高斯模型在N个训练迭代中的不透明度值。如果一个锚点未能生成具有满意不透明度水平的神经高斯模型，我们就将其从场景中移除<br><img src="/2024/02/03/scaffold-gs/Scaffold-GS%5C%E5%9B%BE%E7%89%871.png" alt="梯度（从小到大）多分辨率体素（新锚点着色"><br>锚点细化。我们提出了一种基于误差的锚点生长策略，以可靠地在神经高斯人认为重要的位置生长新的锚点。 我们将神经高斯量化为多分辨率体素，并向梯度大于水平阈值的体素添加新的锚点。 我们的策略有效地提高了场景覆盖率，而不会使用过多的点。我们将卡车场景中的初始锚点和精炼锚点可视化。卡车用圆圈标出。注意，精炼点有效地覆盖了周围区域和精细结构，倾向于更完整和详细的场景渲染。<br><img src="/2024/02/03/scaffold-gs/Scaffold-GS%5C%E5%9B%BE%E7%89%872.png" alt="卡车描点细化效果"><br>结果：Scaffold-GS&nbsp;在各种类型场景上的渲染结果。 包括无纹理区域、观测不足、精细细节、视效和多尺度观察等具有挑战性的情况都能得到合理处理。Scaffold-GS&nbsp;对视图相关效应（例如反射、阴影）更鲁棒; 并减轻冗余 3D 高斯引起的伪影（例如漂浮物、结构误差）</p><h3 id="定量比较"><a href="#定量比较" class="headerlink" title="定量比较"></a>定量比较</h3><p><img src="/2024/02/03/scaffold-gs/Scaffold-GS%5C%E5%9B%BE%E7%89%873.png" alt="复杂数据集效果更好、存储空间更小"></p><h3 id="定性比较"><a href="#定性比较" class="headerlink" title="定性比较"></a>定性比较</h3><p><img src="/2024/02/03/scaffold-gs/Scaffold-GS%5C%E5%9B%BE%E7%89%874.png" alt="卡车描点细化效果"><br>在不同数据集上对Scaffold-GS和3D-GS[22]进行定性比较。强调视觉差异的补丁用箭头和绿色和黄色的插图来强调。我们的方法在这些场景中优于3D-GS，在具有挑战性的场景中具有明显的优势，例如薄几何和细尺度细节，无纹理区域，光效果，观察不足。还可以观察到我们的模型在表示不同尺度和观看距离的内容方面具有优势。</p><iframe height="498" width="510" src="Scaffold-GS\compare_apar_t.mp4"></iframe>]]></content>
      
      
      
        <tags>
            
            <tag> 三维重建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mip-splatting</title>
      <link href="/2024/02/03/mip-splatting/"/>
      <url>/2024/02/03/mip-splatting/</url>
      
        <content type="html"><![CDATA[<p>3D高斯展开（3DGS）展示了令人印象深刻的新视图合成结果，达到了高保真度和高效率。 然而，在改变采样率时，例如通过改变焦距或相机距离(3DGS由于使用膨胀而产生膨胀和侵蚀伪影。 当放大或靠近相机时，它会产生侵蚀效果。 这是因为膨胀的 2D 高斯在屏幕空间中变小，使对象结构比实际看起来更薄。 相反，屏幕空间膨胀在缩小或远离场景时会产生膨胀伪影。 在这种情况下，膨胀的 2D 高斯在屏幕空间中变得更大，使对象结构比实际看起来更厚。)。 我们发现，这种现象的根源可以归因于缺乏3D频率约束和使用2D膨胀滤波器。 为了解决这个问题，我们引入了一个 3D 平滑滤波器，该滤波器根据输入视图引起的最大采样频率来约束 3D 高斯基元的大小(将3D表示的频率限制在训练图像确定的最大采样率的一半以下)，从而消除了放大时的高频伪影。 此外，用模拟 2D 盒式滤波器的 2D Mip滤波器代替 2D 膨胀，可有效缓解混叠和膨胀问题。 我们的综合评估，包括单尺度图像训练和多尺度测试等场景，验证了我们方法的有效性。</p><p><img src="/2024/02/03/mip-splatting/01.png"></p><p>3D 高斯展开通过将 3D 对象表示为高斯投影到图像平面上，然后在屏幕空间中进行 2D 膨胀来渲染图像，如 （a） 所示。 该方法的固有收缩偏差导致退化的 3D 高斯超过了采样极限，如 （b） 中的 δ 函数所示，同时由于膨胀操作而呈现类似于 2D。 然而，当改变采样率（通过焦距或相机距离）时，我们观察到强烈的膨胀效应（c）和高频伪像（d）。</p><p>一句话总结：Mip-Splatting为3D GS引入了3D平滑滤波器和2D Mip滤波器，消除了多个伪影。</p><p><img src="/2024/02/03/mip-splatting/02.png" alt="多尺度训练测试"></p><p>Blender数据集上的多尺度训练和多尺度测试，Mip-Splatting在大多数指标上都达到了最优性能，显著优于3DGS和3DGS+EWA(椭圆加权平均滤波器)。</p><p><img src="/2024/02/03/mip-splatting/03.png" alt="单尺度训练测试"></p><p>在Blender数据集上进行单尺度训练和多尺度测试，所有方法都在全分辨率下训练，然后在较小分辨率下进行评估，以模拟放大效果。Mip-Splatting在低分辨率下优于3DGS和3DGS+EWA。</p><p>本文提出了Mip-Splatting，这是对3D高斯Splatting的修改，引入了两个新颖的滤波器，即3D平滑滤波器和2D Mip滤波器，以在任意尺度实现无混叠的渲染。我们的3D平滑滤波器有效地限制了高斯原语的最大频率，以匹配训练图像施加的采样约束，而2D Mip滤波器则近似盒滤波器以模拟物理成像过程。实验结果表明，在相同的尺度/采样率下进行训练和测试时，Mip-Splatting在性能上可与最先进的方法相竞争。重要的是，在分布不一致的情境下，即在不同于训练的采样率下进行测试时，它在超出分布的摄像机姿势和缩放因子的情况下表现显著优越，实现了更好的泛化效果。</p><p>缺陷：该方法使用高斯滤波器作为盒状滤波器的高效近似。然而，这种近似引入了误差，特别是当高斯在屏幕空间中很小时，增大缩小导致了较大的误差，如表2所示。此外，由于每个3D高斯的采样率必须每m = 100次迭代计算一次，训练开销略有增加。</p><p><img src="/2024/02/03/mip-splatting/04.jpg"></p><p>复现过程</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda create -y -n mip-splatting python=3.8</span><br><span class="line">conda activate mip-splatting</span><br><span class="line">pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html</span><br><span class="line">conda install cudatoolkit-dev=11.3 -c conda-forge</span><br><span class="line"># 使用 Conda 包管理器安装 CUDA 工具包的开发版本（cudatoolkit-dev），并指定安装版本为 11.3</span><br></pre></td></tr></tbody></table></figure><p>conda init bash 是指运行 conda init 命令，以初始化您的 Bash shell(是 Bourne Again SHell 的缩写，是一个流行的 Unix/Linux 环境中常见的命令行解释器。) 以便与 Conda 兼容。这个命令的目的是配置您的 shell(命令解释器，允许用户与操作系统进行交互)，以便能够正确使用 conda activate 等 Conda 命令。</p><p><img src="/2024/02/03/mip-splatting/05.png" alt="激活环境"></p><p><img src="/2024/02/03/mip-splatting/06.png"></p><p><img src="/2024/02/03/mip-splatting/07.png"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install submodules/diff-gaussian-rasterization</span><br><span class="line">pip install submodules/simple-knn/</span><br></pre></td></tr></tbody></table></figure><p>清华大学的 PyPI 镜像，可以使用 -i 参数指定镜像地址：pip install -r requirements.txt -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></p><p><img src="/2024/02/03/mip-splatting/08.png"></p><p>对于Blender数据集从<a href="https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1">https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1</a>下载，仅保留ship数据集，运行</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert_blender_data.py --blender_dir nerf_synthetic/ --object_name ship --out_dir multi-scale</span><br></pre></td></tr></tbody></table></figure><p>得到multi-scale,</p><p><img src="/2024/02/03/mip-splatting/09.png"></p><p><img src="/2024/02/03/mip-splatting/010.png"></p><p>运行多尺度评估和测试，单尺度评估和测试，这里仅使用ship数据集，所以scenes需要修改</p><p>single-scale training and single-scale testing on NeRF-synthetic dataset</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/run_nerf_synthetic_stmt.py </span><br></pre></td></tr></tbody></table></figure><p>multi-scale training and multi-scale testing on NeRF-synthetic dataset</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/run_nerf_synthetic_mtmt.py</span><br></pre></td></tr></tbody></table></figure><p>Online viewer: After training, you can fuse the 3D smoothing filter to the Gaussian parameters with</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python create_fused_ply.py -m {model_dir}/{scene} --output_ply fused/{scene}_fused.ply</span><br></pre></td></tr></tbody></table></figure><p>Then use our <a href="https://niujinshuchong.github.io/mip-splatting-demo">online viewer</a> to visualize the trained model.</p><p><img src="/2024/02/03/mip-splatting/011.png" alt="修改ship"></p><p><img src="/2024/02/03/mip-splatting/012.png"></p><p>运行</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python create_fused_ply.py -m benchmark_nerf_synthetic_ours_mtmt/ship --output_ply fused/ship_fused.ply</span><br></pre></td></tr></tbody></table></figure><p>得到fused中.plt,可在<a href="https://niujinshuchong.github.io/mip-splatting-demo/">https://niujinshuchong.github.io/mip-splatting-demo/</a>导入查看。</p><p>对于mip-nerf 360数据集，先将文件夹名修改为360_v2,代码中默认文件名为360_v2,同样只使用flowers数据集，scenes仅保留flowers,之后同上</p><ul><li>single-scale training and single-scale testing on the mip-nerf 360 dataset</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/run_mipnerf360.py </span><br></pre></td></tr></tbody></table></figure><ul><li>single-scale training and multi-scale testing on the mip-nerf 360 dataset</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/run_mipnerf360_stmt.py</span><br></pre></td></tr></tbody></table></figure><p><img src="/2024/02/03/mip-splatting/013.png"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 三维重建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/02/02/hello-world/"/>
      <url>/2024/02/02/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
