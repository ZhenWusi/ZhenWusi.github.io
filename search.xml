<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>3d gaussian splating</title>
      <link href="/2024/03/30/3dgs/"/>
      <url>/2024/03/30/3dgs/</url>
      
        <content type="html"><![CDATA[<h1 id="3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering"><a href="#3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering" class="headerlink" title="3D Gaussian Splatting for Real-Time Radiance Field Rendering"></a>3D Gaussian Splatting for Real-Time Radiance Field Rendering</h1><p><img src="/2024/03/30/3dgs/fig1.png" alt="Fig 1"></p><details><summary>英文</summary> Fig. 1. Our method achieves real-time rendering of radiance fields with quality that equals the previous method with the best quality [Barron et al. 2022],while only requiring optimization times competitive with the fastest previous methods [Fridovich-Keil and Yu et al. 2022; Müller et al. 2022]. Key to this performance is a novel 3D Gaussian scene representation coupled with a real-time differentiable renderer, which offers significant speedup to both scene optimization and novel view synthesis. Note that for comparable training times to InstantNGP [Müller et al. 2022], we achieve similar quality to theirs; while this is the maximum quality they reach, by training for 51min we achieve state-of-the-art quality, even slightly better than Mip-NeRF360 [Barron et al. 2022]. </details> <details><summary>中文</summary>图1. 我们的方法实现了辐射场的实时渲染，其质量与之前具有最佳质量的方法相当[Barron等人，2022年]，同时只需要与最快的先前方法[Fridovich-Keil和Yu等人，2022年；Müller等人，2022年]竞争力相当的优化时间。这种性能的关键是一种新颖的3D高斯场景表示，与实时可微渲染器相结合，这显著加快了场景优化和新视角合成的速度。值得注意的是，与InstantNGP[Müller等人，2022年]相比，我们在类似的训练时间内实现了类似的质量；虽然这是他们达到的最高质量，但通过51分钟的训练，我们实现了最先进的质量，甚至略优于Mip-NeRF360[Barron等人，2022年]。optimization：最优化，最佳选择；comprtitive:竞争的，一样好的，couple:夫妻，结合，连接；speedup：加速；</details><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><details><summary>英文</summary>       Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates.Weintroduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥ 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration,we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows real time rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.</details><details><summary>中文</summary>辐射场方法最近在多张照片或视频中捕获的场景的新视点合成方面取得了革命性的进展。然而，要实现高质量的视觉效果仍然需要昂贵的神经网络进行训练和渲染，而最近的快速方法不可避免地在速度和质量之间做出权衡。对于无界和完整的场景（而不仅仅是孤立的对象）以及1080p分辨率的渲染，目前没有一种方法可以实现实时的显示速率。我们引入了三个关键要素，使我们能够在保持竞争力的培训时间的同时实现最先进的视觉质量，并且重要的是允许在1080p分辨率下进行高质量的实时（≥ 30 fps）新视点合成。首先，从相机校准期间产生的稀疏点开始，我们使用3D高斯来表示场景，这些高斯保留了连续体积辐射场的理想特性，用于场景优化，同时避免了在空白空间中进行不必要的计算；其次，我们对3D高斯进行交替优化/密度控制，特别是优化各向异性协方差，以实现对场景的准确表示；第三，我们开发了一种快速的可见性感知渲染算法，支持各向异性的喷洒，并且既加速了训练，又允许实时渲染。我们在几个已建立的数据集上展示了最先进的视觉质量和实时渲染。 revolutionized：改变了；</details><h3 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h3>  <details><summary>英文</summary>  Meshes and points are the most common 3D scene representations becausetheyareexplicitandareagoodfitforfastGPU/CUDA-based rasterization. In contrast, recent Neural Radiance Field (NeRF) methods build on continuous scene representations, typically optimizing a Multi-Layer Perceptron (MLP) using volumetric ray-marching for novel-view synthesis of captured scenes. Similarly, the most efficient radiance field solutions to date build on continuous representations by interpolating values stored in, e.g., voxel [Fridovich-Keil and Yu et al. 2022] or hash [Müller et al. 2022] grids or points [Xu et al. 2022]. While the continuous nature of these methods helps optimization, the stochastic sampling required for rendering is costly and can result in noise. We introduce a new approach that combines the best of both worlds: our 3D Gaussian representation allows optimization with state-of-the-art (SOTA) visual quality and competitive training times, while our tile-based splatting solution ensures real-time rendering at SOTA quality for 1080p resolution on several previously published datasets [Barron et al. 2022; Hedman et al. 2018; Knapitsch et al. 2017] (see Fig. 1).</details> <details><summary>中文</summary>网格和点云是最常见的3D场景表示方法，因为它们是显式的且适用于快速的GPU/CUDA渲染。然而，最近的神经辐射场（Neural Radiance Field，简称NeRF）方法采用了连续的场景表示，通常通过体积光线投射来优化多层感知器（Multi-Layer Perceptron，简称MLP），以实现对捕获场景的新视点合成。类似地，迄今为止最高效的辐射场解决方案也是基于连续表示的，例如在体素网格（如[Fridovich-Keil和Yu，2022]）或哈希网格（如[Müller等，2022]）中插值存储的值，或者基于点云（如[Xu等，2022]）。虽然这些方法的连续性有助于优化，但渲染所需的随机采样成本高昂，可能会产生噪音。因此，我们引入了一种新方法，将两者的优点结合起来：我们的3D高斯表示在保持最先进的视觉质量和竞争性培训时间的同时，我们的基于瓦片的喷洒（splatting）解决方案确保了在1080p分辨率下以最先进的质量进行实时渲染，适用于几个先前发布的数据集</details><details><summary>英文</summary> Our goal is to allow real-time rendering for scenes captured with multiple photos, and create the representations with optimization times as fast as the most efficient previous methods for typical real scenes. Recent methods achieve fast training [Fridovich-Keiland Yu et al. 2022; Müller et al. 2022], but struggle to achieve the visual quality obtained by the current SOTA NeRF methods, i.e., Mip-NeRF360 [Barron et al. 2022], which requires up to 48 hours of training time. The fast– but lower-quality– radiance field methods can achieve interactive rendering times depending on the scene (10-15 frames per second), but fall short of real-time rendering at high resolution.</details> <details><summary>中文</summary>我们的目标是实现对使用多张照片捕获的场景进行实时渲染，并在典型真实场景中以与最高效的先前方法相当的优化时间创建表示。最近的方法实现了快速训练1，但在视觉质量上仍无法达到当前SOTA NeRF方法（例如Mip-NeRF360 2），后者需要长达48小时的训练时间。虽然快速但质量较低的辐射场方法可以根据场景实现交互式渲染时间（每秒10-15帧），但无法实现高分辨率下的实时渲染</details><details><summary>英文</summary>Our solution builds on three main components. We first introduce 3D Gaussians as a flexible and expressive scene representation. Westart with the same input as previous NeRF-like methods, i.e., cameras calibrated with Structure-from-Motion (SfM) [Snavely et al. 2006] and initialize the set of 3D Gaussians with the sparse point cloud produced for free as part of the SfM process. In contrast to most point-based solutions that require Multi-View Stereo (MVS) data [Aliev et al. 2020; Kopanas et al. 2021; Rückert et al. 2022], we achieve high-quality results with only SfM points as input. Note that for the NeRF-synthetic dataset, our method achieves high quality even with random initialization. We show that 3D Gaussians are an excellent choice, since they are a differentiable volumetric representation, but they can also be rasterized very efficiently by projecting them to 2D, and applying standard 𝛼-blending, using an equivalent image formation model as NeRF. The second component of our method is optimization of the properties of the 3D Gaussians– 3Dposition, opacity 𝛼, anisotropic covariance, and spherical harmonic (SH) coefficients– interleaved with adaptive density control steps, where we add and occasionally remove 3D Gaussians during optimization. The optimization procedure produces a reasonably compact, unstructured, and precise representation of the scene (1-5 million Gaussians for all scenes tested). The third and final element of our method is our real-time rendering solution that uses fast GPU sorting algorithms and is inspired by tile-based rasterization, following recent work [Lassner and Zollhofer 2021]. However, thanks to our 3D Gaussian representation, we can perform anisotropic splatting that respects visibility ordering– thanks to sorting and 𝛼blending– and enable a fast and accurate backward pass by tracking the traversal of as many sorted splats as required.</details><details><summary>中文</summary>我们的解决方案基于三个主要组件。首先，我们引入了3D高斯作为一种灵活且表达能力强的场景表示方法。我们从与之前类似NeRF方法相同的输入开始，即使用**结构运动（Structure-from-Motion，简称SfM）校准的相机，并使用SfM过程中免费生成的稀疏点云来初始化3D高斯集合。与大多数基于点的解决方案不同，后者需要多视图立体匹配（Multi-View Stereo，简称MVS）**数据，我们仅使用SfM点作为输入就可以获得高质量的结果。值得注意的是，在NeRF合成数据集上，即使随机初始化，我们的方法也可以实现高质量的结果。我们证明了3D高斯是一个出色的选择，因为它们是可微分的体积表示，但也可以通过将其投影到2D并应用标准的𝛼混合（alpha-blending）来高效地进行光栅化，使用与NeRF相同的等效图像形成模型。我们方法的第二个组成部分是对3D高斯的属性进行优化，包括3D位置、不透明度𝛼、各向异性协方差和球谐（SH）系数，这些属性与自适应密度控制步骤交替进行，我们在优化过程中添加并偶尔移除3D高斯。优化过程产生了一个合理紧凑、非结构化且精确的场景表示（在所有测试场景中，使用了100-500万个高斯）。我们方法的第三个和最后一个元素是我们的实时渲染解决方案，它使用快速的GPU排序算法，受到基于瓦片的光栅化的启发，遵循最近的工作1。然而，由于我们的3D高斯表示，我们可以执行各向异性喷洒，以遵守排序和𝛼混合，同时通过跟踪所需数量的已排序喷洒的遍历来实现快速且准确的反向传递。</details><details><summary>英文</summary>To summarize, we provide the following contributions: • Theintroductionofanisotropic3DGaussiansasahigh-quality, unstructured representation of radiance fields. • An optimization method of 3D Gaussian properties, interleaved with adaptive density control that creates high-quality representations for captured scenes. • Afast, differentiable rendering approach for the GPU, which is visibility-aware, allows anisotropic splatting and fast backpropagation to achieve high-quality novel view synthesis. Our results on previously published datasets show that we can optimize our 3D Gaussians from multi-view captures and achieve equal or better quality than the best quality previous implicit radiance f ield approaches. We also can achieve training speeds and quality similar to the fastest methods and importantly provide the first real-time rendering with high quality for novel-view synthesis</details><details><summary>中文</summary>总结一下，我们提供了以下贡献：<ol><li><strong>引入各向异性3D高斯</strong>作为辐射场的高质量、非结构化表示方法。</li><li><strong>优化3D高斯属性</strong>的方法，与自适应密度控制交替进行，为捕获的场景创建高质量的表示。</li><li>基于GPU的<strong>快速、可微分渲染方法</strong>，具有可见性感知性，支持各向异性喷洒，并通过跟踪所需数量的已排序喷洒实现快速且准确的反向传递。</li></ol><p>我们在先前发布的数据集上的结果表明，我们可以从多视图捕获中优化我们的3D高斯，并获得与最佳隐式辐射场方法相等或更好的质量。我们还可以实现与最快方法相似的训练速度和质量，并且重要的是首次提供了具有高质量的实时渲染的新视点合成。</p></details><h3 id="2-RELATEDWORK"><a href="#2-RELATEDWORK" class="headerlink" title="2 RELATEDWORK"></a>2 RELATEDWORK</h3><details><summary>英文</summary><p> We first briefly overview traditional reconstruction, then discuss<br> point-based rendering and radiance field work, discussing their  similarity; radiance fields are a vast area, so we focus only ondirectly<br> related work. For complete coverage of the field, please see the<br> excellent recent surveys [Tewari et al. 2022; Xie et al. 2022].</p></details><details><summary>中文</summary>我们首先简要回顾传统的重建方法，然后讨论基于点的渲染和辐射场工作，探讨它们的相似性；辐射场是一个广阔的领域，因此我们只关注直接相关的工作。要全面了解该领域，请参阅[Tewari et al. 2022; Xie et al. 2022]的优秀最新综述。</details><h4 id="2-1-Traditional-Scene-Reconstruction-and-Rendering"><a href="#2-1-Traditional-Scene-Reconstruction-and-Rendering" class="headerlink" title="2.1 Traditional Scene Reconstruction and Rendering"></a>2.1 Traditional Scene Reconstruction and Rendering</h4><details><summary>英文</summary> The first novel-view synthesis approaches were based on light fields, f irst densely sampled [Gortler et al. 1996; Levoy and Hanrahan 1996] then allowing unstructured capture [Buehler et al. 2001]. The advent of Structure-from-Motion (SfM) [Snavely et al. 2006] enabled an entire new domain where a collection of photos could be used to synthesize novel views. SfM estimates a sparse point cloud during camera calibration, that was initially used for simple visualization of 3D space. Subsequent multi-view stereo (MVS) produced impressive full 3D reconstruction algorithms over the years [Goesele et al. 2007], enabling the development of several view synthesis algorithms [Chaurasia et al. 2013; Eisemann et al. 2008; Hedman et al. 2018; Kopanas et al. 2021]. All these methods re-project and blend the input images into the novel view camera, and use the geometry to guide this re-projection. These methods produced excellent results in many cases, but typically cannot completely recover from unreconstructed regions, or from “over-reconstruction”, when MVS generates inexistent geometry. Recent neural rendering algorithms [Tewari et al. 2022] vastly reduce such artifacts and avoid the overwhelming cost of storing all input images on the GPU, outperforming these methods on most fronts.</details><details><summary>中文</summary>第一个新颖的视图合成方法基于光场，首先是密集采样 [Gortler 等人。 1996； Levoy 和 Hanrahan 1996] 然后允许非结构化捕获 [Buehler 等人。 2001]。运动结构 (SfM) 的出现 [Snavely 等人。 2006]启用了一个全新的领域，可以使用一组照片来合成新颖的视图。 SfM 在相机校准期间估计稀疏点云，最初用于 3D 空间的简单可视化。随后的多视图立体 (MVS) 多年来产生了令人印象深刻的全 3D 重建算法 [Goesele 等人。 2007]，使得多种视图合成算法的开发成为可能[Chaurasia 等人。 2013年；艾斯曼等人。 2008年；赫德曼等人。 2018；科帕纳斯等人。 2021]。所有这些方法都将输入图像重新投影并混合到新颖的视图相机中，并使用几何形状来指导这种重新投影。这些方法在许多情况下产生了出色的结果，但当 MVS 生成不存在的几何体时，通常无法从未重建区域或“过度重建”中完全恢复。最近的神经渲染算法 [Tewari 等人。 2022]大大减少了此类伪影，并避免了将所有输入图像存储在 GPU 上的巨大成本，在大多数方面都优于这些方法。</details><h4 id="2-2-Neural-Rendering-and-Radiance-Fields"><a href="#2-2-Neural-Rendering-and-Radiance-Fields" class="headerlink" title="2.2 Neural Rendering and Radiance Fields"></a>2.2 Neural Rendering and Radiance Fields</h4><details><summary>英文</summary>Deeplearning techniques were adopted early for novel-view synthesis [Flynn et al. 2016; Zhou et al. 2016]; CNNs were used to estimate blendingweights[Hedmanetal.2018],orfortexture-spacesolutions [Riegler and Koltun 2020; Thies et al. 2019]. The use of MVS-based geometry is a major drawback of most of these methods; in addition, the use of CNNs for final rendering frequently results in temporal f lickering. Volumetric representations for novel-view synthesis were initiated by Soft3D [Penner and Zhang 2017]; deep-learning techniques coupled with volumetric ray-marching were subsequently proposed[Henzleretal.2019;Sitzmannetal.2019]buildingonacontinuous differentiable density field to represent geometry. Rendering using volumetric ray-marching has a significant cost due to the large number of samples required to query the volume. Neural Radiance Fields (NeRFs) [Mildenhall et al. 2020] introduced importance sampling and positional encoding to improve quality, but used a large Multi-Layer Perceptron negatively affecting speed. The success of NeRFhasresulted in anexplosion of follow-up methods that address quality and speed, often by introducing regularization strategies; the current state-of-the-art in image quality for novel-view synthesis is Mip-NeRF360 [Barron et al. 2022]. While the rendering quality is outstanding, training and rendering times remain extremely high; we are able to equal or in some cases surpass this quality while providing fast training and real-time rendering.</details><details><summary>中文</summary>深度学习技术早期被用于新视角合成¹²。卷积神经网络（CNN）被用于估计融合权重³，或者用于纹理空间解决方案⁴。然而，大多数这些方法都使用基于多视图立体几何的表示，这是一个主要的缺点。此外，使用CNN进行最终渲染通常会导致时间上的闪烁。<p>体素表示用于新视角合成的初始方法是Soft3D⁵。随后，深度学习技术与体素光线投射相结合，构建了一个连续可微的密度场来表示几何形状⁶。使用体素光线投射进行渲染由于需要大量样本来查询体积而成本高昂。神经辐射场（NeRF）⁷ 引入了重要性采样和位置编码以提高质量，但使用了大型多层感知器（MLP），从而影响了速度。NeRF的成功导致了一系列后续方法的涌现，这些方法通常通过引入正则化策略来解决质量和速度问题。目前，新视角合成的图像质量的最新技术是Mip-NeRF360⁸。虽然渲染质量出色，但训练和渲染时间仍然非常长。我们能够在提供快速训练和实时渲染的同时，达到或甚至超越这一质量水平。</p></details><details><summary>英文</summary>The most recent methods have focused on faster training and/or rendering mostly by exploiting three design choices: the use of spatial data structures to store (neural) features that are subsequently interpolated during volumetric ray-marching, different encodings,and MLP capacity. Such methods include different variants of space discretization [Chen et al. 2022b,a; Fridovich-Keil and Yu et al. 2022; Garbin et al. 2021; Hedman et al. 2021; Reiser et al. 2021; Takikawa et al. 2021; Wu et al. 2022; Yu et al. 2021], codebooks [Takikawa et al. 2022], and encodings such as hash tables [Müller et al. 2022], allowing the use of a smaller MLP or foregoing neural networks completely [Fridovich-Keil and Yu et al. 2022; Sun et al. 2022]. Mostnotable of these methods are InstantNGP[Müller et al. 2022] which uses a hash grid and an occupancy grid to accelerate computation and a smaller MLP to represent density and appearance; and Plenoxels [Fridovich-Keil and Yu et al. 2022] that use a sparse voxel grid to interpolate a continuous density field, and are able to forgo neural networks altogether. Both rely on Spherical Harmonics: the former to represent directional effects directly, the latter to encode its inputs to the color network. While both provide outstanding results, these methods can still struggle to represent empty space effectively, depending in part on the scene/capture type. In addition, image quality is limited in large part by the choice of the structured grids used for acceleration, and rendering speed is hindered by the need to query many samples for a given ray-marching step. The unstructured, explicit GPU-friendly 3D Gaussians weuseachievefaster rendering speed and better quality without neural components.</details><details><summary>中文</summary>最近的方法主要集中在更快的训练和/或渲染，主要通过利用三个设计选择来实现：使用空间数据结构来存储（神经）特征，随后在体素光线投射期间进行插值，不同的编码和多层感知器（MLP）容量。这些方法包括不同变体的空间离散化⁵⁶⁷⁸    ，码本，以及哈希表等编码，允许使用较小的MLP或完全放弃神经网络 。其中最值得注意的方法包括InstantNGP，它使用哈希网格和占用网格来加速计算，并使用较小的MLP来表示密度和外观；以及Plenoxels，它使用稀疏体素网格来插值连续的密度场，并且能够完全放弃神经网络。这两种方法都依赖于球谐函数：前者直接表示方向效应，后者将其输入编码到颜色网络中。虽然两者都提供了出色的结果，但这些方法在一定程度上仍然难以有效地表示空白空间，这部分取决于场景/捕捉类型。此外，图像质量在很大程度上受到用于加速的结构化网格的选择的限制，而渲染速度受到查询给定光线投射步骤的许多样本的需求的影响。我们使用的非结构化、明确的GPU友好的3D高斯函数实现了更快的渲染速度和更好的质量，而无需神经组件。</details><h4 id="2-3-Point-Based-Rendering-and-Radiance-Fields"><a href="#2-3-Point-Based-Rendering-and-Radiance-Fields" class="headerlink" title="2.3 Point-Based Rendering and Radiance Fields"></a>2.3 Point-Based Rendering and Radiance Fields</h4><details><summary>英文</summary>Point-based methods efficiently render disconnected and unstructured geometry samples (i.e., point clouds) [Gross and Pfister 2011]. In its simplest form, point sample rendering [Grossman and Dally 1998] rasterizes an unstructured set of points with a fixed size, for whichit mayexploitnatively supported point types of graphics APIs [Sainz and Pajarola 2004] or parallel software rasterization on the GPU[Laine and Karras 2011; Schütz et al. 2022]. While true to the underlying data, point sample rendering suffers from holes, causes aliasing, and is strictly discontinuous. Seminal work on high-quality point-based rendering addresses these issues by “splatting” point primitives with an extent larger than a pixel, e.g., circular or elliptic discs, ellipsoids, or surfels [Botsch et al. 2005; Pfister et al. 2000; Ren et al. 2002; Zwicker et al. 2001b]. Therehasbeenrecentinterestindifferentiable point-based rendering techniques [Wiles et al. 2020; Yifan et al. 2019]. Points have been augmented with neural features and rendered using a CNN [Aliev et al. 2020; Rückert et al. 2022] resulting in fast or even real-time view synthesis; however they still depend on MVS for the initial geometry, and as such inherit its artifacts, most notably over- or under-reconstruction in hard cases such as featureless/shiny areas or thin structures.</details><details><summary>中文</summary>点云方法高效地渲染了不连续和非结构化的几何样本（即点云）¹。在其最简单的形式中，点样本渲染²将一个非结构化的点集进行光栅化，其大小固定，可以利用图形API的本地支持的点类型³，或者在GPU上进行并行软件光栅化⁴。虽然点样本渲染忠实于底层数据，但它存在孔洞、导致混叠，并且严格不连续。关于高质量点云渲染的开创性工作通过“喷洒”比像素大的点基元来解决这些问题，例如圆形或椭圆形的圆盘、椭球体或surfels⁵   。<p>近年来，不可微分的基于点的渲染技术引起了人们的兴趣 。点被增强为具有神经特征，并使用CNN进行渲染，从而实现了快速甚至实时的视图合成；然而，它们仍然依赖于多视图立体几何（MVS）来获得初始几何形状，并因此继承了其众所周知的缺陷，尤其是在无特征/闪亮区域或薄结构等复杂情况下的过度或不足重建。</p></details><details><summary>中英文</summary>Point-based 𝛼-blending and NeRF-style volumetric rendering share essentially the same image formation model. Specifically, the color 𝐶 is given by volumetric rendering along a ray:<p>点云方法的𝛼混合和NeRF风格的体积渲染实际上共享相同的图像形成模型。具体而言，颜色𝐶是通过沿着一条射线进行体积渲染来确定的：</p><p>$\ C = \sum_{i=1}^{N} \left( T_i (1 - \exp(-\sigma_i \delta_i))c_i \right)    \quad(1)$<br>其中：<br>$ T_i = \exp \left( -\sum_{j=1}^{i-1} \sigma_j \delta_j \right)$</p><ul><li>$ N $表示沿射线的样本数。</li><li>$ T_i $  是透射率。</li><li>$ sigma_i $ 表示介质的密度。</li><li>$ delta_i $表示沿射线的间隔。</li><li>$ c_i $对应于每个采样点的颜色。</li></ul><p> where samples of density 𝜎, transmittance𝑇, and color c are taken<br> along the ray with intervals 𝛿𝑖. This can be re-written as</p><p> 在沿着射线以间隔𝛿𝑖取样密度𝜎、透射率𝑇和颜色𝑐的样本。这可以重新表述为</p><p>$ \sum_{i=1}^{N} T_i \alpha_i c_i  \quad(2)$</p><p>其中：</p><ul><li>$ \alpha_i = (1 - \exp(-\sigma_i \delta_i)) $</li><li>$ T_i = \prod_{j=1}^{i-1} (1 - \alpha_i)\ $</li></ul><p>Atypical neural point-based approach (e.g., [Kopanas et al. 2022,<br> 2021]) computes the color𝐶 of a pixel by blending N ordered points<br> overlapping the pixel:</p><p>典型的神经点云方法（例如，[Kopanas等人，2022年；2021年]）通过混合重叠在像素上的N个有序点来计算像素的颜色𝐶：</p><p>$ C = \sum_{i \in N} \left( c_i \alpha_i \prod_{j=1}^{i-1} (1 - \alpha_j) \right) \quad(3)$</p><p>where c𝑖 is the color of each point and 𝛼𝑖 is given by evaluating a<br> 2D Gaussian with covariance Σ [Yifan et al. 2019] multiplied with a<br> learned per-point opacity.<br>其中，$c_i$ 表示每个点的颜色，$alpha_i$ 是通过评估具有协方差 $\Sigma$ 的二维高斯函数并乘以每个点的透明度得出的。</p></details><details><summary>英文</summary>From Eq. 2 and Eq. 3, we can clearly see that the image formation model is the same. However, the rendering algorithm is very different. NeRFs are a continuous representation implicitly representing empty/occupied space; expensive random sampling is required to f ind the samples in Eq. 2 with consequent noise and computational expense. In contrast, points are an unstructured, discrete representation that is flexible enough to allow creation, destruction, and displacement of geometry similar to NeRF. This is achieved by optimizing opacity and positions, as shown by previous work [Kopanas et al. 2021], while avoiding the shortcomings of a full volumetric representation. Pulsar [Lassner and Zollhofer 2021] achieves fast sphere rasterization which inspired our tile-based and sorting renderer. However, given the analysis above, we want to maintain (approximate) conventional 𝛼-blending on sorted splats to have the advantages of volumetric representations: Our rasterization respects visibility order in contrast to their order-independent method. In addition, we backpropagate gradients on all splats in a pixel and rasterize anisotropic splats. These elements all contribute to the high visual quality of our results (see Sec. 7.3). In addition, previous methods mentioned above also use CNNs for rendering, which results in temporal instability. Nonetheless, the rendering speed of Pulsar [Lassner and Zollhofer 2021] and ADOP[Rückertetal.2022]servedasmotivation to develop our fast rendering solution.</details><details><summary>中文</summary>从公式2和公式3中，我们可以清楚地看到图像形成模型是相同的。然而，渲染算法却非常不同。NeRF（神经辐射场）是一种连续表示，隐式地表示了空/占用空间；在公式2中，需要昂贵的随机采样，从而产生噪音和计算开销。相比之下，点云是一种非结构化、离散的表示，足够灵活，可以允许类似NeRF的几何形状的创建、销毁和位移。这是通过优化不透明度和位置实现的，正如之前的工作所示[Kopanas等人，2021年]，同时避免了完整体积表示的缺点。<p>Pulsar [Lassner和Zollhofer，2021年]实现了快速的球体光栅化，这启发了我们基于瓦片和排序的渲染器。然而，鉴于上述分析，我们希望保持对排序的斑点进行（近似）常规𝛼混合，以具有体积表示的优势：我们的光栅化与其无序方法相比尊重可见性顺序。此外，我们在像素中反向传播所有斑点的梯度，并光栅化各向异性斑点。所有这些因素都有助于我们结果的高视觉质量（请参阅第7.3节）。此外，上述提到的先前方法还使用CNN进行渲染，这导致了时间上的不稳定性。尽管如此，Pulsar [Lassner和Zollhofer，2021年]和ADOP[Rückertetal.2022]的渲染速度成为我们开发快速渲染解决方案的动力。🌟</p></details><details><summary>英文</summary>While focusing on specular effects, the diffuse point-based rendering track of Neural Point Catacaustics [Kopanas et al. 2022] overcomes this temporal instability by using an MLP, but still required MVS geometry as input. The most recent method [Zhang et al. 2022] in this category does not require MVS, and also uses SH for directions; however, it can only handle scenes of one object and needs masks for initialization. While fast for small resolutions and low point counts, it is unclear how it can scale to scenes of typical datasets [Barron et al. 2022; Hedman et al. 2018; Knapitsch et al. 2017]. We use 3D Gaussians for a more flexible scene representation, avoiding the need for MVS geometry and achieving real-time rendering thanks to our tile-based rendering algorithm for the projected Gaussians.A recent approach [Xu et al. 2022] uses points to represent a radiance field with a radial basis function approach. They employ point pruning and densification techniques during optimization, but use volumetric ray-marching and cannot achieve real-time display rates. In the domain of human performance capture, 3D Gaussians have been used to represent captured human bodies [Rhodin et al. 2015; Stoll et al. 2011]; more recently they have been used with volumetric ray-marching for vision tasks [Wang et al. 2023]. Neural volumetric primitives have been proposed in a similar context [Lombardi et al. 2021]. While these methods inspired the choice of 3D Gaussians as our scene representation, they focus on the specific case of reconstructing and rendering a single isolated object (a human body or face), resulting in scenes with small depth complexity. In contrast, our optimization of anisotropic covariance, our interleaved optimization/density control, and efficient depth sorting for rendering allow us to handle complete, complex scenes including background, both indoors and outdoors and with large depth complexity.</details><details><summary>中文</summary>在关注镜面效果时，神经点光线追踪的漫反射点云渲染跟踪（例如Neural Point Catacaustics [Kopanas等人，2022年]）通过使用MLP（多层感知器）克服了时间上的不稳定性，但仍需要多视图立体几何（MVS）作为输入。在这一类别中，最近的方法[Zhang等人，2022年]不需要MVS，还使用了球谐函数（SH）来表示方向；然而，它只能处理一个物体的场景，并且需要用于初始化的遮罩。虽然对于小分辨率和低点数的情况下速度很快，但它尚不清楚如何扩展到典型数据集的场景[Barron等人，2022年；Hedman等人，2018年；Knapitsch等人，2017年]。我们使用三维高斯函数来实现更灵活的场景表示，避免了对MVS几何的需求，并通过我们的基于瓦片的渲染算法实现了投影高斯函数的实时渲染。<p>最近的一种方法[Xu等人，2022年]使用点来表示具有径向基函数方法的辐射场。他们在优化过程中采用了点修剪和加密技术，但使用了体积光线行进，并且无法实现实时显示速率。</p><p>在人体性能捕捉领域，三维高斯函数已被用于表示捕捉到的人体[Rhodin等人，2015年；Stoll等人，2011年]；最近，它们已与体积光线行进一起用于视觉任务[Wang等人，2023年]。神经体积原语在类似的背景下也被提出[Lombardi等人，2021年]。尽管这些方法启发了我们选择三维高斯函数作为场景表示，但它们专注于重建和渲染单个孤立对象（例如人体或面部），从而导致深度复杂度较小的场景。相比之下，我们对各向异性协方差的优化、交错的优化/密度控制以及用于渲染的高效深度排序使我们能够处理完整、复杂的场景，包括室内外背景，以及具有大深度复杂度的场景。</p></details><h3 id="3-OVERVIEW"><a href="#3-OVERVIEW" class="headerlink" title="3 OVERVIEW"></a>3 OVERVIEW</h3><details><summary>英文</summary>The input to our method is a set of images of a static scene, together with the corresponding cameras calibrated by SfM [Schönberger and Frahm 2016] which produces a sparse point cloud as a sideeffect. From these points we create a set of 3D Gaussians (Sec. 4), defined by a position (mean), covariance matrix and opacity 𝛼, that allows a very flexible optimization regime. This results in a reasonably compact representation of the 3D scene, in part because highly anisotropic volumetric splats can be used to represent fine structures compactly. The directional appearance component (color) of the radiance field is represented via spherical harmonics (SH), following standard practice [Fridovich-Keil and Yu et al. 2022; Müller et al. 2022]. Our algorithm proceeds to create the radiance field representation (Sec. 5) via a sequence of optimization steps of 3D Gaussian parameters, i.e., position, covariance, 𝛼 and SH coefficients interleaved with operations for adaptive control of the Gaussian density. The key to the efficiency of our method is our tile-based rasterizer (Sec. 6) that allows 𝛼-blending of anisotropic splats, respecting visibility order thanks to fast sorting. Out fast rasterizer also includes a fast backward pass by tracking accumulated 𝛼 values, without a limit on the number of Gaussians that can receive gradients. The overview of our method is illustrated in Fig. 2.</details><details><summary>中文</summary>我们方法的输入是一组静态场景的图像，以及由SfM（结构光束法）校准的相应相机，这会产生一个稀疏的点云作为副作用。从这些点中，我们创建了一组三维高斯函数（第4节），由位置（均值）、协方差矩阵和不透明度𝛼定义，从而实现了非常灵活的优化方案。这导致了对3D场景的相对紧凑的表示，部分原因是高度各向异性的体积斑点可以紧凑地表示精细结构。辐射场的方向外观成分（颜色）通过球谐函数（SH）表示，遵循标准做法[Fridovich-Keil和Yu等人，2022年；Müller等人，2022年]。我们的算法通过一系列对3D高斯参数的优化步骤（即位置、协方差、𝛼和SH系数）来创建辐射场表示（第5节），并与自适应控制高斯密度的操作交错进行。我们方法效率的关键在于我们基于瓦片的光栅化器（第6节），它允许各向异性斑点的𝛼混合，通过快速排序尊重可见性顺序。我们的快速光栅化器还包括通过跟踪累积的𝛼值进行快速反向传播，而不限制可以接收梯度的高斯数量。我们方法的概述如图2所示。</details><h3 id="4-DIFFERENTIABLE-3D-GAUSSIAN-SPLATTING"><a href="#4-DIFFERENTIABLE-3D-GAUSSIAN-SPLATTING" class="headerlink" title="4 DIFFERENTIABLE 3D GAUSSIAN SPLATTING"></a>4 DIFFERENTIABLE 3D GAUSSIAN SPLATTING</h3><details><summary>中英文</summary> Our goal is to optimize a scene representation that allows highquality novel view synthesis, starting from a sparse set of (SfM) points without normals. To do this, we need a primitive that inherits the properties of differentiable volumetric representations, while at the same time being unstructured and explicit to allow very fast rendering. We choose 3D Gaussians, which are differentiable and can be easily projected to 2D splats allowing fast 𝛼-blending for rendering. Our representation has similarities to previous methods that use 2D points [Kopanas et al. 2021; Yifan et al. 2019] and assume each point is a small planar circle with a normal. Given the extreme sparsity of SfM points it is very hard to estimate normals. Similarly, optimizing very noisy normals from such an estimation would be very challenging. Instead, we model the geometry as a set of 3D Gaussians that do not require normals. Our Gaussians are defined by a full 3D covariance matrix Σ defined in world space [Zwicker et al. 2001a] centered at point (mean) 𝜇:<p> 我们的目标是优化一种场景表示，允许从稀疏的（SfM）点集开始进行高质量的新视角合成，而无需法线。为了实现这一目标，我们需要一种既具有可微分体积表示的特性，同时又是非结构化且明确的，以便实现非常快速的渲染。我们选择了三维高斯函数，它们是可微分的，并且可以轻松投影到二维斑点，从而实现了快速的𝛼混合渲染。</p><p>我们的表示与之前使用2D点的方法[Kopanas等人，2021年；Yifan等人，2019年]有相似之处，并假设每个点是一个带有法线的小平面圆。鉴于SfM点的极端稀疏性，很难估计法线。类似地，从这种估计中优化非常嘈杂的法线将是非常具有挑战性的。因此，我们将几何形状建模为一组不需要法线的三维高斯函数。我们的高斯函数由在世界空间中定义的完整三维协方差矩阵Σ定义，以点（均值）𝜇为中心[Zwicker等人，2001a年]：</p><p>$G(x) = e^{-\frac{1}{2} (x)^T \Sigma^{-1} (x)}\quad(4)$</p><p>This Gaussian function is multiplied by $\alpha$ in our blending process.<br>这个高斯函数在我们的混合过程中与 $\alpha$ 相乘</p><p>However,we need to project our 3D Gaussians to 2D for rendering.<br>Zwicker et al. [2001a] demonstrate how to do this projection to image space. Given a viewing transformation 𝑊 the covariance matrix Σ′ in camera coordinates is given as follows:</p><p>然而，我们需要将我们的三维高斯函数投影到二维以进行渲染。Zwicker等人[2001a]演示了如何将其投影到图像空间。给定一个视图变换𝑊，协方差矩阵Σ′在相机坐标中定义如下：</p><p> $ \Sigma’ = J W \Sigma W^T J^T \quad(5)$</p><p> where 𝐽 is the Jacobian of the affine approximation of the projective<br> transformation. Zwicker et al. [2001a] also show that if we skip the<br> third row and column of Σ′, we obtain a 2×2 variance matrix with<br> the same structure and properties as if we would start from planar<br> points with normals, as in previous work [Kopanas et al. 2021].<br> 其中，𝐽是投影变换的仿射近似的雅可比矩阵。Zwicker等人[2001a]还表明，如果我们跳过Σ′的第三行和第三列，我们将获得一个2×2的方差矩阵，其结构和性质与我们从具有法线的平面点开始的情况相同，就像之前的工作[Kopanas等人，2021年]一样<br> An obvious approach would be to directly optimize the covariance<br> matrix Σ to obtain 3D Gaussians that represent the radiance field.<br> However, covariance matrices have physical meaning only when<br> they are positive semi-definite. For our optimization of all our pa<br>rameters, we use gradient descent that cannot be easily constrained<br> to produce such valid matrices, and update steps and gradients can<br> very easily create invalid covariance matrices.<br> 一个明显的方法是直接优化协方差矩阵Σ，以获得表示辐射场的三维高斯函数。然而，协方差矩阵只有在它们是半正定的时候才具有物理意义。对于我们所有参数的优化，我们使用梯度下降，很难约束其产生这样有效的矩阵，而且更新步骤和梯度很容易产生无效的协方差矩阵。<br> As a result, we opted for a more intuitive, yet equivalently ex<br>pressive representation for optimization. The covariance matrix Σ<br> of a 3D Gaussian is analogous to describing the configuration of an<br> ellipsoid. Given a scaling matrix 𝑆 and rotation matrix 𝑅, we can<br> f<br> ind the corresponding Σ:<br> 因此，我们选择了一种更直观但同样表达能力的优化表示。三维高斯函数的协方差矩阵Σ类似于描述椭球体的配置。给定一个缩放矩阵𝑆和旋转矩阵𝑅，我们可以找到相应的Σ：<br> $ \Sigma = R S S^T R^T \quad(6) $<br> To allow independent optimization of both factors, we store them<br> separately: a 3D vector 𝑠 for scaling and a quaternion 𝑞 to represent<br> rotation. These canbetrivially converted to their respective matrices<br> and combined, making sure to normalize 𝑞 to obtain a valid unit<br> quaternion.<br> 为了允许独立优化这两个因素，我们将它们分开存储：一个用于缩放的三维向量𝑠和一个用于表示旋转的四元数𝑞。这些可以轻松地转换为各自的矩阵并组合，确保归一化𝑞以获得有效的单位四元数。<br> To avoid significant overhead due to automatic differentiation<br> during training, we derive the gradients for all parameters explicitly.<br> Details of the exact derivative computations are in appendix A.<br> 为了避免由于训练过程中的自动微分而产生的显著开销，我们明确地计算了所有参数的梯度。关于精确导数计算的详细信息请参见附录A。<br> This representation of anisotropic covariance– suitable for op<br>timization– allows us to optimize 3D Gaussians to adapt to the<br> geometry of different shapes in captured scenes, resulting in a fairly<br> compact representation. Fig. 3 illustrates such cases.<br> 这种适用于优化的各向异性协方差表示使我们能够优化三维高斯函数以适应捕获场景中不同形状的几何结构，从而得到相对紧凑的表示。图3说明了这样的情况。</p></details><h4 id="5-OPTIMIZATION-WITH-ADAPTIVE-DENSITY-CONTROL-OF-3D-GAUSSIANS"><a href="#5-OPTIMIZATION-WITH-ADAPTIVE-DENSITY-CONTROL-OF-3D-GAUSSIANS" class="headerlink" title="5 OPTIMIZATION WITH ADAPTIVE DENSITY CONTROL OF 3D GAUSSIANS"></a>5 OPTIMIZATION WITH ADAPTIVE DENSITY CONTROL OF 3D GAUSSIANS</h4><p><img src="/2024/03/30/3dgs/fig2.png" alt="Fig 2"><br>图2. 优化从稀疏的SfM点云开始，创建了一组3D高斯函数。然后，我们对这组高斯函数的密度进行优化和自适应控制。在优化过程中，我们使用了快速的基于瓦片的渲染器，使其在训练时间上具有与SOTA快速辐射场方法相竞争的优势。一旦训练完成，我们的渲染器允许对各种场景进行实时导航.<br>Projection：投射；Adaptive Density Control：自适应密度控制；可微分瓦片光栅化器（Differentiable Tile Rasterizer）；操作流程（Operation Flow）；梯度流（Gradient Flow）</p><p><img src="/2024/03/30/3dgs/fig3.png" alt="Fig 3"><br>图3。我们在优化后将3D高斯函数缩小60%（最右侧）。这清楚地展示了经过优化后紧凑地表示复杂几何形状的3D高斯函数的各向异性形状。左侧是实际渲染的图像。</p><details><summary>英文</summary>The core of our approach is the optimization step, which creates a dense set of 3D Gaussians accurately representing the scene for free-view synthesis. In addition to positions 𝑝, 𝛼, and covariance Σ, we also optimize SH coefficients representing color 𝑐 of each Gaussian to correctly capture the view-dependent appearance of the scene. The optimization of these parameters is interleaved with steps that control the density of the Gaussians to better represent the scene.</details><details><summary>中文</summary>我们方法的核心是优化步骤，它创建了一组密集的三维高斯函数，准确地表示了用于自由视角合成的场景。除了位置𝑝、𝛼和协方差Σ之外，我们还优化了表示每个高斯函数颜色𝑐的球谐系数，以正确捕捉场景的视角相关外观。这些参数的优化与控制高斯函数密度的步骤交错进行，以更好地表示场景。</details><h4 id="5-1-Optimization"><a href="#5-1-Optimization" class="headerlink" title="5.1 Optimization"></a>5.1 Optimization</h4><details><summary>英文</summary>The optimization is based on successive iterations of rendering and comparing the resulting image to the training views in the captured dataset. Inevitably, geometry may be incorrectly placed due to the ambiguities of 3D to 2D projection. Our optimization thus needs to be able to create geometry and also destroy or move geometry if it has been incorrectly positioned. The quality of the parameters of the covariances of the 3D Gaussians is critical for the compactness of the representation since large homogeneous areas can be captured with a small number of large anisotropic Gaussians. Weuse Stochastic Gradient Descent techniques for optimization, taking full advantage of standard GPU-accelerated frameworks, and the ability to add custom CUDA kernels for some operations, following recent best practice [Fridovich-Keil and Yu et al. 2022; Sun et al. 2022]. In particular, our fast rasterization (see Sec. 6) is critical in the efficiency of our optimization, since it is the main computational bottleneck of the optimization</details><details><summary>中文</summary>优化基于连续的渲染和将生成的图像与捕获数据集中的训练视图进行比较的迭代。由于3D到2D投影的不确定性，几何体可能被错误地放置。因此，我们的优化需要能够创建几何体，同时如果其位置不正确，还可以销毁或移动几何体。3D高斯函数的协方差参数的质量对于表示的紧凑性至关重要，因为大均匀区域可以用少量大型各向异性高斯函数捕获。<p>我们使用随机梯度下降技术进行优化，充分利用标准的GPU加速框架，并且可以为某些操作添加自定义的CUDA内核，遵循最近的最佳实践¹²。特别是，我们快速的光栅化（见第6节）对于我们的优化效率至关重要，因为它是优化的主要计算瓶颈。</p></details><details><summary>英文</summary></details>]]></content>
      
      
      <categories>
          
          <category> 三维重建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3d gaussian splating </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>STL常用知识点</title>
      <link href="/2024/03/05/stl-chang-yong-zhi-shi-dian/"/>
      <url>/2024/03/05/stl-chang-yong-zhi-shi-dian/</url>
      
        <content type="html"><![CDATA[<h1 id="STL-标准模版本库-常用知识点总结"><a href="#STL-标准模版本库-常用知识点总结" class="headerlink" title="STL(标准模版本库)常用知识点总结"></a>STL(标准模版本库)常用知识点总结</h1><h2 id="vector-向量-用法"><a href="#vector-向量-用法" class="headerlink" title="vector(向量)用法"></a>vector(向量)用法</h2><h3 id="1-定义"><a href="#1-定义" class="headerlink" title="1.定义"></a>1.定义</h3><p>使用vector，需要添加头文件#include &lt;vector&gt;。<br>单独定义一个vector: vector&lt;typename&gt; name;<br>相当于是一维数组name[SIZE]，只不过长度可以变化，和一维数组一样，typename可以是任何数据类型，例如int、char、double、结构体、也可以是STL标准容器，例如vector、set、queue，需要注意的是，如果typename也是一个STL容器，定义的时候需要在&gt;&gt;符号之间加上空格。因为C++ 11之前标准的编译器会把它视为移位操作。如果typename是vector，如vector&lt;vector&lt;int&gt; &gt; name。  </p><p>二维数组是一维是一个数组的数组，vector数组也是一样的，Arrayname[]中的每一个元素都是一个vector，可以看成两个维都可变的二维数组。定义vector数组 vector&lt;typename&gt; Arrayname[arraySizw];（例如vector&lt;int&gt; vi[100]这样Arrayname[0]~Arrayname[arraySize-1]中每一个都是一个vector容器，与vector&lt;vector&lt;int&gt; &gt;name不同的是，这种写法一维长度已经固定为arraySize,另一维才是变长。</p><ul><li>vector&lt;int&gt; a(10); //定义了10个整型元素的向量（尖括号中为元素类型名，它可以是任何合法的数据类型），但没有给出初值，其值是不确定的。</li><li><font color="red">vector&lt;int&gt; a(10,1); //定义了10个整型元素的向量,且给出每个元素的初值为1</font></li><li>vector&lt;int&gt; a(b); //用b向量来创建a向量，整体复制性赋值</li><li>vector&lt;int&gt; a(b.begin(),b.begin+3); //定义了a值为b中第0个到第2个（共3个）元素</li><li>int b[7]={1,2,3,4,5,9,8}; vector&lt;int&gt; a(b,b+7); //从数组中获得初值</li></ul><h3 id="2-基本操作"><a href="#2-基本操作" class="headerlink" title="2.基本操作"></a>2.基本操作</h3><ul><li>a.pop_back(); //删除a向量的最后一个元素</li><li>a.push_back(5); //在a的最后一个向量后插入一个元素，其值为5</li><li><font color="red"><strong>a.back(); //返回a的最后一个元素</strong></font></li><li><font color="red"><strong>a.front(); //返回a的第一个元素</strong></font></li><li>a[i]; //返回a的第i个元素，当且仅当a[i]存在</li><li>a.clear(); //清空a中的元素</li><li>a.empty(); //判断a是否为空，空则返回ture,不空则返回false</li><li>a.assign(b.begin(), b.begin()+3); //b为向量，将b的0~2个元素构成的向量赋给a</li><li>a.assign(4,2); //是a只含4个元素，且每个元素为2</li><li>a.erase(a.begin()+1,a.begin()+3); //删除a中第1个（从第0个算起）到第2个元素，也就是说删除的元素从a.begin()+1算起（包括它）一直到a.begin()+3（不包括它）<font color="red"><strong>用于删除指定位置元素</strong></font></li><li>a.insert(a.begin()+1,5); //在a的第1个元素（从第0个算起）的位置插入数值5，如a为1,2,3,4，插入元素后为1,5,2,3,4<font color="red"><strong>用于向指定位置插入元素</strong></font></li><li>a.insert(a.begin()+1,3,5); //在a的第1个元素（从第0个算起）的位置插入3个数，其值都为5</li><li>a.insert(a.begin()+1,b+3,b+6); //b为数组，在a的第1个元素（从第0个算起）的位置插入b的第3个元素到第5个元素（不包括b+6），如b为1,2,3,4,5,9,8         ，插入元素后为1,4,5,9,2,3,4,5,9,8</li><li><font color="red"><strong>a.size(); //返回a中元素的个数；</strong></font></li><li>a.capacity(); //返回a在内存中总共可以容纳的元素个数</li><li>a.resize(10); //将a的现有元素个数调至10个，多则删，少则补，其值随机</li><li>a.resize(10,2); //将a的现有元素个数调至10个，多则删，少则补，其值为2</li><li>a.reserve(100); //将a的容量（capacity）扩充至100，也就是说现在测试a.capacity();的时候返回值是100.这种操作只有在需要给a添加大量数据的时候才显得有意义，因为这将避免内存多次容量扩充操作（当a的容量不足时电脑会自动扩容，当然这必然降低性能） </li><li><font color="red"><strong>a.swap(b); //b为向量，将a中的元素和b中的元素进行整体性交换</strong></font></li><li>a==b; //b为向量，向量的比较操作还有!=,&gt;=,&lt;=,&gt;,&lt;</li><li>find() 函数来查找指定值的元素，或者使用迭代器来遍历查找。</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">①使用 find() 函数查找：</span><br><span class="line">vector&lt;int&gt; myVector = { 100,200,300,400,500,600 };</span><br><span class="line">vector&lt;int&gt;::iterator it = find(myVector.begin(), myVector.end(), 500);</span><br><span class="line">//输出内容为：目标元素的索引为: 4</span><br><span class="line">if (it != myVector.end()) {</span><br><span class="line">cout &lt;&lt; "目标元素的索引为: " &lt;&lt; distance(myVector.begin(), it) &lt;&lt;endl;</span><br><span class="line">}</span><br><span class="line">else {</span><br><span class="line">cout &lt;&lt; "没有找到" &lt;&lt;endl;</span><br><span class="line">}</span><br><span class="line">②使用迭代器遍历查找：</span><br><span class="line">vector&lt;int&gt; myVector = { 100,200,300,400,500,600 };</span><br><span class="line">bool found = false;</span><br><span class="line">int valueToFind = 300;</span><br><span class="line">//输出内容为：目标元素的索引为: 2</span><br><span class="line">for (vector&lt;int&gt;::iterator it = myVector.begin(); it != myVector.end(); ++it) {</span><br><span class="line">if (*it == valueToFind) {</span><br><span class="line">cout &lt;&lt; "目标元素的索引为: " &lt;&lt; distance(myVector.begin(), it) &lt;&lt; endl;</span><br><span class="line">found = true;</span><br><span class="line">break;</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">if (!found) {</span><br><span class="line">cout &lt;&lt; "没有找到" &lt;&lt; endl;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><ul><li>访问方法</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cout&lt;&lt;"直接利用数组："; </span><br><span class="line">for(int i=0;i&lt;10;i++)//方法一 </span><br><span class="line">{</span><br><span class="line">    cout&lt;&lt;obj[i]&lt;&lt;" ";</span><br><span class="line">}</span><br><span class="line">cout&lt;&lt;endl; </span><br><span class="line">cout&lt;&lt;"利用迭代器：" ;</span><br><span class="line">//方法二，使用迭代器(iterator)将容器中数据输出 </span><br><span class="line">vector&lt;int&gt;::iterator it;//声明一个迭代器(类似一个指针)，来访问vector容器，作用：遍历或者指向vector容器的元素 </span><br><span class="line">for(it=obj.begin();it!=obj.end();it++)</span><br><span class="line">{</span><br><span class="line">    cout&lt;&lt;*it&lt;&lt;" ";</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><ul><li>定义五行六列二维数组</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//法一</span><br><span class="line">vector&lt;vector&lt;int&gt; &gt; obj(5); //定义二维动态数组大小5行 </span><br><span class="line">    for(int i =0; i&lt; obj.size(); i++)//动态二维数组为5行6列，值全为0 </span><br><span class="line">    { </span><br><span class="line">        obj[i].resize(6); </span><br><span class="line">    } </span><br><span class="line">//法二</span><br><span class="line">vector&lt;vector&lt;int&gt; &gt; obj(5, vector&lt;int&gt;(6)); //定义二维动态数组5行6列 </span><br></pre></td></tr></tbody></table></figure><p>几种重要的算法，使用时需要包含头文件：#include<algorithm>  </algorithm></p><ul><li><font color="red" size="4"><strong>sort(a.begin(),a.end())</strong></font>; // 对a中的从a.begin()（包括它）到a.end()（不包括它）的元素进行从小到大排列</li><li><font color="red" size="4"><strong>reverse(a.begin(),a.end())</strong></font>; // 对a中的从a.begin()（包括它）到a.end()（不包括它）的元素倒置，但不排列，如a中元素为1,3,2,4,倒置后为4,2,3,1</li><li><font color="red" size="4"><strong>copy(a.begin(),a.end(),b.begin()+1)</strong></font>; // 把a中的从a.begin()（包括它）到a.end()（不包括它）的元素复制到b中，从b.begin()+1的位置（包括它）开始复制，覆盖掉原有元素</li><li><font color="red" size="4"><strong>find(a.begin(),a.end(),10)</strong></font>; // 在a中的从a.begin()（包括它）到a.end()（不包括它）的元素中查找10，若存在返回其在向量中的位置</li></ul><h2 id="set-集合-用法"><a href="#set-集合-用法" class="headerlink" title="set(集合)用法"></a>set(集合)用法</h2><h3 id="1-定义-1"><a href="#1-定义-1" class="headerlink" title="1.定义"></a>1.定义</h3><p>使用set需要添加头文件，#include &lt;set&gt;<br>单独定义set&lt;typename&gt; name,typename可以是任何类型，结构体、STL标准容器也可以<br>set&lt;Type&gt; s(s1)              //定义一个set容器，并用容器s1来初始化<br>set&lt;Type&gt; s(b, e)  //b和e分别为迭代器的开始和结束的标记<br>set&lt;Type&gt; s(s1.begin(), s1.begin()+3) //用容器s1的第0个到第2个值初始化s<br>set&lt;Type&gt; s(a, a + 5)            //将a数组的元素初始化vec向量，不包括a[4]<br>set&lt;Type&gt; s(&amp;a[1], &amp;a[4])   //将a[1]~a[4]范围内的元素作为s的初始值</p><ul><li>每个元素的键值都唯一，不允许两个元素有相同的键值。</li><li>所有元素都会根据元素的键值自动排序（默认从小到大）。</li><li>set 中的元素不像 map 那样可以同时拥有实值(value)和键值(key)，只能存储键，是单纯的键的集合。</li><li>set 中元素的值不能直接被改变。</li><li>set 支持大部分的map的操作，但是 set 不支持下标的操作，而且没有定义mapped_type类型。</li></ul><h3 id="2-基本操作-1"><a href="#2-基本操作-1" class="headerlink" title="2.基本操作"></a>2.基本操作</h3><ul><li>set只能通过迭代器iterator访问，除string和vector外的STL容器都不支持*(it+i)的访问形式</li><li>s.begin()//返回指向第一个元素的迭代器</li><li>s.end()//返回指向最后一个元素的迭代器</li><li>s.clear()//清除所有元素</li><li>s.count(值)//返回某个值元素的个数</li><li>s.empty()//如果集合为空，返回true，否则返回false</li><li>s.equal_range()//返回集合中与给定值相等的上下限的两个迭代器</li><li>s.erase()//删除集合中的元素</li><li>s.find(k)//返回一个指向被查找到元素的迭代器</li><li><font color="red"><strong>s.insert()//在集合中插入元素,并自动排序和去重</strong></font></li><li>s.lower_bound(k)//返回一个迭代器，指向键值大于等于k的第一个元素</li><li>s.upper_bound(k)//返回一个迭代器，指向键值大于k的第一个元素</li><li>s.max_size()//返回集合能容纳的元素的最大限值</li><li>s.rbegin()//返回指向集合中最后一个元素的反向迭代器</li><li>s.rend()//返回指向集合中第一个元素的反向迭代器</li><li><font color="red"><strong>s.size()//集合中元素的数目</strong></font></li><li>erase(iterator) ：删除定位器iterator指向的值,iterator为迭代器，可通过find函数查找</li><li>erase(first,second)：删除定位器first和second之间的值</li><li>erase(key_value)：删除键值key_value的值，key_value为所需要删除元素的值</li><li>find用法<font color="red"><strong>返回set中对应值为value的迭代器</strong></font></li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">//返回一个指向被查找到元素的迭代器，如果没找到则返回end()</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;set&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main(){</span><br><span class="line">     int a[] = {4,5,6};</span><br><span class="line">     set&lt;int&gt; s(a,a+3);</span><br><span class="line">     set&lt;int&gt;::iterator it;</span><br><span class="line">     if((it=s.find(4))!=s.end())</span><br><span class="line">         cout&lt;&lt;*it&lt;&lt;endl;</span><br><span class="line">     return 0;</span><br><span class="line">}</span><br><span class="line">输出结果：4</span><br></pre></td></tr></tbody></table></figure><ul><li>insert() 的用法</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">insert(key_value)：将key_value插入到set中 ，返回值是pair\&lt;set::iterator,bool&gt;，bool标志着插入是否成功，而iterator代表插入的位置；若key_value已经在set中，则iterator表示的key_value在set中的位置。  </span><br><span class="line">inset(first,second);将定位器first到second之间的元素插入到set中，返回值是void.</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;set&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main(){</span><br><span class="line">     int a[] = {1,2,3};</span><br><span class="line">     set&lt;int&gt; s;</span><br><span class="line">     set&lt;int&gt;::iterator it;</span><br><span class="line">     s.insert(a,a+3);</span><br><span class="line">     for(it=s.begin(); it!=s.end(); ++it)</span><br><span class="line">         cout&lt;&lt;*it&lt;&lt;" ";</span><br><span class="line">     cout&lt;&lt;endl;</span><br><span class="line">     pair&lt;set&lt;int&gt;::iterator,bool&gt; pr;</span><br><span class="line">     pr=s.insert(5);</span><br><span class="line">     if(pr.second)</span><br><span class="line">         cout&lt;&lt;*pr.first&lt;&lt;endl;</span><br><span class="line">     cout&lt;&lt;"插入数值之后的set中有："&lt;&lt;endl;</span><br><span class="line"> for(it=s.begin(); it!=s.end(); ++it)</span><br><span class="line">cout&lt;&lt;*it&lt;&lt;" ";</span><br><span class="line"> cout&lt;&lt;endl;</span><br><span class="line">     return 0;</span><br><span class="line">}</span><br><span class="line">输出结果：</span><br><span class="line">1 2 3</span><br><span class="line">5</span><br><span class="line">插入数值之后的set中有：</span><br><span class="line">1 2 3 5</span><br></pre></td></tr></tbody></table></figure><ul><li>lower_bound()、upper_bound() 的用法</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">lower_bound(key_value) ：返回第一个大于等于key_value的定位器</span><br><span class="line">upper_bound(key_value)：返回第一个大于key_value的定位器</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;set&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main(){</span><br><span class="line">     set&lt;int&gt; s;</span><br><span class="line">     s.insert(1);</span><br><span class="line">     s.insert(3);</span><br><span class="line">     s.insert(4);</span><br><span class="line">     s.insert(6);</span><br><span class="line">     cout&lt;&lt;*s.lower_bound(1)&lt;&lt;endl;</span><br><span class="line">     cout&lt;&lt;*s.lower_bound(2)&lt;&lt;endl;</span><br><span class="line">     cout&lt;&lt;*s.upper_bound(3)&lt;&lt;endl;</span><br><span class="line">     return 0;</span><br><span class="line">}</span><br><span class="line">输出结果</span><br><span class="line">1</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td></tr></tbody></table></figure><ul><li>equal_range() 的用法</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">equal_range()：返回一对定位器，分别表示第一个大于等于给定关键值的元素和第一个大于给定关键值的元素，这个返回值是一个pair类型。如果这一对定位器中哪个返回失败，就会等于end()的值</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;set&gt;</span><br><span class="line">#include &lt;algorithm&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main(){</span><br><span class="line">set&lt;int&gt; s;</span><br><span class="line">set&lt;int&gt;::iterator it;</span><br><span class="line">for(int i=1; i&lt;=5; ++i)</span><br><span class="line">s.insert(i); //向set中加入数据</span><br><span class="line">for(it=s.begin(); it!=s.end(); ++it)</span><br><span class="line">cout&lt;&lt;*it&lt;&lt;" ";  //输出set中的数据</span><br><span class="line">cout&lt;&lt;endl;</span><br><span class="line">pair&lt;set&lt;int&gt;::const_iterator,set&lt;int&gt;::const_iterator&gt; pr;</span><br><span class="line">pr=s.equal_range(3);</span><br><span class="line">cout&lt;&lt;"第一个大于等于3的数是："&lt;&lt;*pr.first&lt;&lt;endl;</span><br><span class="line">cout&lt;&lt;"第一个大于3的数是： "&lt;&lt;*pr.second&lt;&lt;endl;</span><br><span class="line">return 0;</span><br><span class="line">}</span><br><span class="line">输出结果：</span><br><span class="line">1 2 3 4 5</span><br><span class="line">第一个大于等于3的数是：3</span><br><span class="line">第一个大于3的数是：4</span><br></pre></td></tr></tbody></table></figure><ul><li>补充<br>set中元素唯一，处理不唯一情况用multiset<br>只去重不排序用unordered_set</li></ul><h2 id="string用法"><a href="#string用法" class="headerlink" title="string用法"></a>string用法</h2><h3 id="1-定义-2"><a href="#1-定义-2" class="headerlink" title="1.定义"></a>1.定义</h3><p>使用string，需要添加#include&lt;string&gt;,// 注意这里不是string.h，string.h是C字符串头文件</p><ol><li>string s;  // 生成一个空字符串s </li><li>string s(str) ; // 拷贝构造函数生成str的复制品 </li><li>string s(str, stridx);  // 将字符串str内”始于位置stridx”的部分当作字符串的初值 </li><li>string s(str, stridx, strlen) ; // 将字符串str内”始于stridx且长度顶多strlen”的部分作为字符串的初值 </li><li>string s(cstr) ;  // 将C字符串（以NULL结束）作为s的初值 </li><li>string s(chars, chars_len) ;  // 将C字符串前chars_len个字符作为字符串s的初值。 </li><li>string s(num, ‘c’) ;  // 生成一个字符串，包含num个c字符 </li><li>string s(“value”);  string s=“value”;  // 将s初始化为一个字符串字面值副本</li><li>string s(begin, end);  // 以区间begin/end(不包含end)内的字符作为字符串s的初值 </li><li>s.~string();  //销毁所有字符，释放内存 </li><li>string串要取得其中某一个字符，和传统的C字符串一样，可以用s[i]的方式取得。比较不一样的是如果s有三个字符，传统C的字符串的s[3]是’\0’字符，但是C++的string则是只到s[2]这个字符而已。C风格字符串<br>用”“括起来的字符串常量，C++中的字符串常量由编译器在末尾添加一个空字符；末尾添加了‘\0’的字符数组，C风格字符串的末尾必须有一个’\0’。C字符数组及其与string串的区别<br>char ch[ ]={‘C’, ‘+’, ‘+’}; //末尾无NULL<br>char ch[ ]={‘C’, ‘+’, ‘+’, ‘\0’}; //末尾显式添加NULL<br>char ch[ ]=”C++”; //末尾自动添加NULL字符 若[ ]内数字大于实际字符数，将实际字符存入数组，其余位置全部为’\0’。</li><li>输入输出整个字符串只能用cin和cout,用printf输出需要用c_str()将string类型转为字符数组进行输出，如</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">string str="abcd"</span><br><span class="line">printf("%s\n",str.c_str());//将string型str使用c_str()变为字符数组</span><br></pre></td></tr></tbody></table></figure><center><table><thead><tr><th align="center">操作</th><th align="center">string</th><th align="center">字符阵列</th></tr></thead><tbody><tr><td align="center">声明字符串</td><td align="center">string s;</td><td align="center">char s[100]</td></tr><tr><td align="center">取得第i个字符串</td><td align="center">s[i]</td><td align="center">s[i]</td></tr><tr><td align="center">字符串长度</td><td align="center"><strong>s.length();s.size();</strong></td><td align="center"><strong>strlen(s)不计\0</strong></td></tr><tr><td align="center">读取一行</td><td align="center">getline(cin,s);</td><td align="center">get(s);</td></tr><tr><td align="center">设成字符串</td><td align="center">s=”TCGS”;</td><td align="center">strcpy(s,”TCGS”);</td></tr><tr><td align="center">字符串相加</td><td align="center">s=s+”TCGS”,s+=”TCGS”</td><td align="center">strcat(s,”TCGS”);</td></tr><tr><td align="center">字符串比较</td><td align="center">s==”TCGS”</td><td align="center">strcmp(s,”TCGS”)</td></tr><tr><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><h3 id="2-string对象的操作"><a href="#2-string对象的操作" class="headerlink" title="2.string对象的操作"></a>2.string对象的操作</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">string s;</span><br><span class="line">1)  s.empty();  // s为空串 返回true</span><br><span class="line">2)  s.size();  // 返回s中字符个数 类型应为：string::size_type</span><br><span class="line">3)  s[n];  // 从0开始相当于下标访问</span><br><span class="line">4)  s1+s2;  // 把s1和s2连接成新串 返回新串 </span><br><span class="line">5)  s1=s2;  // 把s1替换为s2的副本</span><br><span class="line">6)  v1==v2;  // 比较，相等返回true</span><br><span class="line">7)  `!=, &lt;, &lt;=, &gt;, &gt;=`  惯有操作 任何一个大写字母都小于任意的小写字母,比较规则是字典序</span><br><span class="line">string s1(“hello”);</span><br><span class="line">string s3=s1+”world”;  //合法操作</span><br><span class="line">string s4=”hello”+”world”;  //非法操作：两个字符串字面值相加</span><br></pre></td></tr></tbody></table></figure><h3 id="3-字符串操作函数"><a href="#3-字符串操作函数" class="headerlink" title="3.字符串操作函数"></a>3.字符串操作函数</h3><ol><li>=, s.assign() // 赋以新值</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">s.assign(str,1,3); // 如果str是"iamangel" 就是把"ama"赋给字符串 </span><br><span class="line">s.assign(str,2,string::npos); // 把字符串str从索引值2开始到结尾赋给s </span><br><span class="line">s.assign("nico",5); // 把’n’ ‘I’ ‘c’ ‘o’ ‘\0’赋给字符串 </span><br><span class="line">s.assign(5,'x'); // 把五个x赋给字符串</span><br></pre></td></tr></tbody></table></figure><ol start="2"><li>swap() // 交换两个字符串的内容 </li><li>+=, s.append(), s.push_back() // 在尾部添加字符 </li><li>s.insert() // 插入字符</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">insert(pos,string) //在pos位置插入字符串string</span><br><span class="line">str.insert(3,str2)//往str[3]处插入str2</span><br><span class="line">insert(it,it2,it3)//串[it2,it3)将备插在it位置上</span><br></pre></td></tr></tbody></table></figure><ol start="5"><li>s.erase() // 删除字符</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">str.erase(it)//删除单个元素</span><br><span class="line">str.erase(first,last)//删除[first,last)区间内所有元素</span><br><span class="line">str.erase(str.begin()+2,str.end()-1)</span><br><span class="line">str.erase(pos,length)//pos为需要开始删除的起始位置，length为删除的字符个数</span><br></pre></td></tr></tbody></table></figure><ol start="6"><li>s.clear() // 删除全部字符 </li><li>s.replace() // 替换字符 </li><li><ul><li>// 串联字符串</li></ul></li><li>==,!=,&lt;,&lt;=,&gt;,&gt;=,compare() // 比较字符串 </li><li>size(),length() // 返回字符数量,两个等效</li><li>max_size() // 返回字符的可能最大个数 </li><li>s.empty() // 判断字符串是否为空 </li><li>s.capacity() // 返回重新分配之前的字符容量 </li><li>reserve() // 保留一定量内存以容纳一定数量的字符 </li><li>[ ], at() // 存取单一字符 </li><li><font color="red"><strong>&gt;&gt;,getline() // 从stream读取某值 ’</strong></font></li><li><font color="red">**&lt;&lt; // 将谋值写入stream ’**</font></li><li>copy() // 将某值赋值为一个C_string </li><li><font color="red"><strong>c_str() // 返回一个指向正规C字符串(C_string)的指针 内容与本string串相同 有’\0’</strong></font></li><li>data() // 将内容以字符数组形式返回 无’\0’ </li><li><font color="red"><strong>s.substr(pos,len) // 返回pos号开始，长度为len的子字符串</strong></font></li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s.substr(); // 返回s的全部内容 </span><br><span class="line">s.substr(11); // 从索引11往后的子串 </span><br><span class="line">s.substr(5,6); // 从索引5开始6个字符</span><br></pre></td></tr></tbody></table></figure><ol start="22"><li>begin() end() // 提供类似STL的迭代器支持 </li><li>rbegin() rend() // 逆向迭代器 </li><li>get_allocator() // 返回配置器</li><li>string::npos//常数，本身值为-1，无符号整型，也可以是4294967285，作为find函数失配时的返回值。</li><li><font color="red"><strong>find() //str.find(str2,pos)，从str的pos号位开始匹配str2,str2是str子串时返回第一次出现位置，不是子串返回string:npos，无pos从头开始。</strong></font></li><li><font color="red"><strong>replace() //str.replace(pos,len,str2)把str从pos号位开始，长度为len的子串替换为str2,str.replace(it1,it2,str2)把str的迭代器[it1，it2)范围的子串替换为str2</strong></font></li><li>字符串流stringstream操作<br>Iostream标准库支持内存中的输入输出，只要将流与存储在程序内存中的string对象捆绑起来即可。此时，可使用iostream输入和输出操作符读写这个stream对象。</li></ol><ul><li>string s</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;操作符 // 用于从istream对象中读入输入</span><br><span class="line">is &gt;&gt; s;  // 从输入流is中读取一个以空白字符分割的字符串，写入s</span><br><span class="line">&lt;&lt;操作符 // 用于把输出写到ostream对象中</span><br><span class="line">os &lt;&lt; s; // 将s写到输出流os中</span><br><span class="line">getline(is, s);  // 从输入流is中读取一行字符，写入s，直到遇到分行符或到了文件尾</span><br><span class="line">istream // 输入流 提供输入操作</span><br><span class="line">ostream // 输出流 提供输出操作</span><br></pre></td></tr></tbody></table></figure><ul><li>stringstream特定的操作</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">stringstream strm; // 创建自由的stringstream对象</span><br><span class="line">stringstream strm(s); // 创建存储s的副本的stringstream对象，s是stringstream类型</span><br><span class="line">strm.str(); // 返回strm中存储的string类型对象</span><br><span class="line">strm.str(s); // 将string类型的s复制给strm 返回void</span><br></pre></td></tr></tbody></table></figure><ul><li>string到int的转换<br>stringstream通常是用来做数据转换的，在多次转换中使用同一个stringstream对象，每次转换前要使用clear()方法。</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">与其他类型间的转换一样大同小异</span><br><span class="line">string result=”10000”;  </span><br><span class="line">int n=0;</span><br><span class="line">stream&lt;&lt;result;</span><br><span class="line">stream&gt;&gt;n;  // n等于10000</span><br></pre></td></tr></tbody></table></figure><ol start="29"><li>C字符串、string串、stringstream之间的关系</li></ol><blockquote><ul><li>string转换成const char *</li></ul><blockquote><p>如果要将字面值string直接转换成const char *类型。string有2个函数可以运用：一个是.c_str()，一个是data成员函数。 c_str()函数返回一个指向正规C字符串的指针，内容与本string串相同。这是为了与C语言兼容，在C语言中没有string类型，故必须通过string类对象的成员函数c_str()把string 对象转换成C中的字符串样式。注意：一定要使用strcpy()函数等来操作方法c_str()返回的指针</p></blockquote></blockquote><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">string str = "Hello World";</span><br><span class="line">const char *ch1 = str.c_str();</span><br><span class="line">const char *ch2 = str.data();</span><br><span class="line">此时，ch1与ch2的内容将都是”Hello World”。但是只能转换成const char*，如果去掉const编译不能通过。</span><br></pre></td></tr></tbody></table></figure><blockquote><ul><li>string转换成char *</li></ul><blockquote><p>C++提供的由C++字符串得到对应的C_string的方法是使用data()、c_str()和copy()，其中 </p><ol><li>data()以字符数组的形式返回字符串内容，但并不添加’\0’。 </li><li>c_str()返回一个以’\0’结尾的字符数组，返回值是const char*。 </li><li>copy()则把字符串的内容复制或写入既有的c_string或字符数组内。<br> C++字符串并不以’\0’结尾。我的建议是在程序中能使用C++字符串就使用，除非万不得已不选用c_string。<br> 如果要转换成char*，可以用string的一个成员函数strcpy实现。</li></ol></blockquote></blockquote><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">string str = "Hello World";</span><br><span class="line">int len = str.length();</span><br><span class="line">char *data = new char[len+1];  //这里+1还是不+1需要注意</span><br><span class="line">strcpy(data, str.c_str());  // const char *data = new char[len+1];  strcpy(data, str);</span><br><span class="line">此时，data中的内容为”Hello World”使用c_str()要么str赋给一个const指针，要么用strcpy()复制。</span><br></pre></td></tr></tbody></table></figure><blockquote><ul><li>char *转换成string</li></ul><blockquote><p>string类型能够自动将C风格的字符串转换成string对象：<br>string str;<br>const char *pc = “Hello World”;<br>str = pc;<br>printf(“%s\n”, str);  //此处出现错误的输出<br>cout&lt;&lt;str&lt;&lt;endl;<br>不过这个是会出现问题的。有一种情况我要说明一下。当我们定义了一个string类型之后，用printf(“%s”,str);输出是会出问题的。这是因为“%s”要求后面的对象的首地址。但是string不是这样的一个类型。所以肯定出错。<br>用cout输出是没有问题的，若一定要printf输出。那么可以这样：<br>printf(“%s”,str.c_str());</p></blockquote></blockquote><blockquote><ul><li>char[ ] 转换成string</li></ul><blockquote><p>这个与char*的情况相同，也可以直接赋值，但是也会出现上面的问题，需要同样的处理。 </p></blockquote><ul><li>字符数组转化成string类型：</li></ul></blockquote><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">char ch [] = "ABCDEFG";</span><br><span class="line">string str(ch); //也可string str = ch;</span><br><span class="line">或者</span><br><span class="line">char ch [] = "ABCDEFG";</span><br><span class="line">string str;</span><br><span class="line">str = ch; //在原有基础上添加可以用str += ch;</span><br></pre></td></tr></tbody></table></figure><blockquote><ul><li>string转换成char[ ]<br> string对象转换成C风格的字符串：<br>const char *str = s.c_str();<br>这是因为为了防止字符数组被程序直接处理c_str()返回了一个指向常量数组的指针。 由于我们知道string的长度可以根据length()函数得到，又可以根据下标直接访问，所以用一个循环就可以赋值了，这样的转换不可以直接赋值。</li></ul></blockquote><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">string str = "Hello World";</span><br><span class="line">int len=str.length();</span><br><span class="line">char ch[255]={};</span><br><span class="line">for( int i=0;i&lt;str.length();i++)</span><br><span class="line">ch[i] = str[i];</span><br><span class="line">ch[len+1] = '\0';</span><br><span class="line">printf("%s\n", ch);</span><br><span class="line">cout&lt;&lt;ch&lt;&lt;endl;</span><br></pre></td></tr></tbody></table></figure><blockquote><ul><li>stringstream与string间的绑定</li></ul></blockquote><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">stringstream strm;</span><br><span class="line">string s;</span><br><span class="line">strm&lt;&lt;s;  // 将s写入到strm</span><br><span class="line">strm&gt;&gt;s;  // 从strm读取串写入s</span><br><span class="line">strm.str(); // 返回strm中存储的string类型对象</span><br><span class="line">strm.str(s); // 将string类型的s复制给strm 返回void</span><br><span class="line">char* cstr;  // 将C字符数组转换成流</span><br><span class="line">string str(cstr);</span><br><span class="line">stringstream ss(str);</span><br></pre></td></tr></tbody></table></figure><h2 id="map-映射-用法"><a href="#map-映射-用法" class="headerlink" title="map(映射)用法"></a>map(映射)用法</h2><h3 id="1-定义-3"><a href="#1-定义-3" class="headerlink" title="1.定义"></a>1.定义</h3><p>头文件，#include&lt;map&gt;<br>单独定义一个map&lt;typename1,typename2&gt; mp;<br>map和其他STL容器在定义上有点不一样，因为map需要确定映射前的类型(键key)和映射后的类型(值value),所以需要在&lt;&gt;内填写两个类型，其中第一个是键的类型，第二个是值的类型。如果是int型映射到int型，就相当于是普通的int型数组。  </p><p>字符串到整数型的映射，必须使用string而不能用char数组。<br>map&lt;string,int&gt; mp;<br>这是因为char数组作为数组，是不能被作为键值的。如果想用字符串做映射，必须使用string。<br>map的键和值也可以是STL容器，如map&lt;set&lt;int&gt;,string&gt; mp;</p><h3 id="2-map容器内元素的访问"><a href="#2-map容器内元素的访问" class="headerlink" title="2.map容器内元素的访问"></a>2.map容器内元素的访问</h3><ul><li>通过下标访问<br>和访问普通数组一样，例如对一个定义为map&lt;char,int&gt; mp的map来说，就可以直接使用mp[‘c’]的方式来访问它对应的整数。应注意，map中键的值是唯一的。  </li><li>通过迭代器访问<br>map迭代器的定义和其他STL容器的迭代器定义方式相同：<br>ma&lt;typename1,typename2&gt;::itrator it;<br>map迭代器使用方式和其他STL容器有所不同，因为map的每一对映射都有两个typename，这决定了必须能通过一个it来同时访问键和值。事实上，,map可以使用it-&gt;first来访问键，使用it-&gt;second来访问值。<font color="red">map会以键从小到大的顺序自动排序，这是由于map内部是红黑树实现的(set也是)，在建立的过程中会自动实现从小到大的排序功能。</font></li></ul><h3 id="3-map常用函数解析"><a href="#3-map常用函数解析" class="headerlink" title="3.map常用函数解析"></a>3.map常用函数解析</h3><ol><li>find(key)  // 返回键为key 的映射的迭代器，如map&lt;char,int&gt;::itrator it=mp.find(‘b’)</li><li>erase()  // mp.erase(it) it为需要删除的元素的迭代器;mp.erase(key) key为需要删除元素的键;mp.erase(first,last) first为要删除区间的起始迭代器，last为需要删除区间的末尾迭代器的下一个地址。</li><li>size() //获取map中映射的对数</li><li>clear() //清空map中所有元素</li></ol><h2 id="queue-队列-用法"><a href="#queue-队列-用法" class="headerlink" title="queue(队列)用法"></a>queue(队列)用法</h2><h3 id="1-定义-4"><a href="#1-定义-4" class="headerlink" title="1.定义"></a>1.定义</h3><p>添加头文件 #include&lt;queue&gt;<br>queue&lt;typename&gt;name;//typename可以是任意基本数据类型或容器</p><h3 id="2-queue容器内元素的访问"><a href="#2-queue容器内元素的访问" class="headerlink" title="2.queue容器内元素的访问"></a>2.queue容器内元素的访问</h3><p>由于队列(queue)本身就是一种先进先出的限制性数据结构，因此在STL中只能通过front()来访问队首元素，或是通过back()来访问队尾元素。</p><h3 id="3-queue常用函数解析"><a href="#3-queue常用函数解析" class="headerlink" title="3.queue常用函数解析"></a>3.queue常用函数解析</h3><ol><li>push(x)//将x进行入队</li><li>pop(x)//令队首元素出队</li><li>empty()//检测queue是否为空</li><li>size()//返回queue内元素的个数</li></ol><h2 id="priority-queue-优先队列-用法"><a href="#priority-queue-优先队列-用法" class="headerlink" title="priority_queue(优先队列)用法"></a>priority_queue(优先队列)用法</h2></center>]]></content>
      
      
      
        <tags>
            
            <tag> C++语言 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scaffold-GS</title>
      <link href="/2024/02/03/scaffold-gs/"/>
      <url>/2024/02/03/scaffold-gs/</url>
      
        <content type="html"><![CDATA[<h1 id="Scaffold-GS-Structured-3D-Gaussians-for-View-Adaptive-Rendering-用于视图自适应渲染的结构化3D高斯函数"><a href="#Scaffold-GS-Structured-3D-Gaussians-for-View-Adaptive-Rendering-用于视图自适应渲染的结构化3D高斯函数" class="headerlink" title="Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering(用于视图自适应渲染的结构化3D高斯函数)"></a>Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering(用于视图自适应渲染的结构化3D高斯函数)</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><font size="2">Neural rendering methods have significantly advanced photo-realistic 3D scene rendering in various academic and industrial applications. The recent 3D Gaussian Splatting method has achieved state-of-the-art rendering quality and speed by combining the benefits of both primitive-based representations and volumetric representations. However, it often leads to heavily redundant Gaussians that try to fit every training view, neglecting the underlying scene geometry. Consequently, the resulting model becomes less robust to significant view changes, texture-less areas, and lighting effects.We introduce Scaffold-GS, which uses anchor points to distribute local 3D Gaussians and predicts their attributes on-the-fly based on viewing direction and distance within the view frustum. Anchor growing and pruning strategies are developed based on the importance of neural Gaussians to reliably improve the scene coverage. We show that our method effectively reduces redundant Gaussians while delivering high-quality rendering. We also demonstrate an enhanced capability to accommodate scenes with varying levels-of-detail and view-dependent observations, without sacrificing rendering speed.</font></p><p>神经渲染方法在各种学术和工业应用中显着提高了照片级真实感 3D 场景渲染的性能。 最近的 3D 高斯喷射方法结合了基于基元(primitive,基本单元)表示和体积表示的优势，实现了最先进的渲染质量和速度。 然而，它通常会导致严重冗余的高斯模型，试图适应每个训练视图，而忽略了底层的场景几何形状。 因此，生成的模型对于显着的视图变化、无纹理区域和照明效果变得不太稳健。 我们引入了 Scaffold-GS，<font color="Red"><strong>它使用锚点来分布局部3D 高斯，并根据视锥体内的观察方向和距离动态预测它们的属性</strong></font>。 锚点生长和修剪策略是基于神经高斯模型对于可靠地提高场景覆盖范围的重要性而开发的。 我们表明，我们的方法有效地减少了冗余高斯，同时提供高质量的渲染。 <font color="Red"><strong>我们还展示了一种增强的能力，可以适应具有不同细节级别和依赖于视图的观察的场景，而无需牺牲渲染速度。</strong></font></p><hr><p>使用一组在双层层次结构中结构的3D高斯函数来表示场景。锚定在初始点的稀疏网格上，从每个锚点衍生出一组适度的神经高斯函数，以动态适应不同的视角和距离。该方法在具有挑战性观察视图的场景中表现出色。例如，透明度、镜面反射、反射、无纹理区域和精细比例细节。<br><img src="/2024/02/03/scaffold-gs/Scaffold-GS%5Cteaser_big.png" alt="复杂视图与3dgs对比"></p><hr><h2 id="本文贡献如下："><a href="#本文贡献如下：" class="headerlink" title="本文贡献如下："></a>本文贡献如下：</h2><p>1)利用场景结构，我们从<font color="Red"><strong>稀疏体素网格中初始化锚点来指导局部三维高斯分布</strong></font>，形成分层和区域感知的场景表示;</p><p>2)在视锥内，我们<font color="Red"><strong>实时预测每个锚点的神经高斯分布，以适应不同的观看方向和距离，</strong></font>从而产生更鲁棒的新视图合成;</p><p>3)我们开发了一个更可靠的<font color="Red"><strong>锚点生长和修剪策略，利用预测的神经高斯来更好地覆盖场景。</strong></font></p><h2 id="原理："><a href="#原理：" class="headerlink" title="原理："></a>原理：</h2><p>我们首先从 SFM(运动恢复结构) 衍生的点形成一个稀疏体素(voxel)网格。每个可见体素的中心都放置有一个可学习尺度(learnable offset)的锚点(anchor)，大致雕刻了场景的占用情况。（b） 视锥内，从每个可见的带有偏移量𝑂_𝑘锚点生成 k 个神经高斯。它们的属性，即不透明度(opacity)、颜色、比例和四元数(quaternion)是使用MLP从锚点特征、相对相机-锚点的观看方向和距离进行解码,使用 Fα、Fc、Fs、Fq表示。 （c）神经高斯模型属性的预测是即时的，意味着只有在视锥体内可见的锚点才会被激活以生成神经高斯模型。为了使栅格化更加高效，我们仅保留透明度值大于预定义阈值 τα 的神经高斯模型。这大大降低了计算负载，并帮助我们的方法保持与原始3D-GS相当的高渲染速度。通过重建L_1、结构相似性L_𝑆𝑆𝐼𝑀和体积正则化L_𝑣𝑜𝑙来监督渲染图像。<br><img src="/2024/02/03/scaffold-gs/Scaffold-GS%5Cpipeline.png" alt="原理图"></p><h3 id="描点细化"><a href="#描点细化" class="headerlink" title="描点细化"></a>描点细化</h3><p>增长操作：由于神经高斯模型与它们的锚点紧密关联，而<font color="Red"><strong>这些锚点是从SFM点初始化的，它们的建模能力受限于局部区域，特别是在无纹理和少观察的区域。</strong></font>因此，我们提出了一种基于误差的锚点增长策略，即在神经高斯模型发现显著的地方增加新的锚点。为了确定显著的区域，我们首先通过构建大小为ϵg的体素来空间量化神经高斯模型。对于每个体素，我们计算包含的神经高斯模型在N个训练迭代中的平均梯度，表示为∇g。然后，对于∇g &gt; τg的体素，我们认为其是显著的，其中τg是预定义的阈值；<font color="Red"><strong>如果该体素中没有已建立的锚点，则在该体素的中心部署一个新的锚点。</strong></font>图3说明了这个增长操作。在实践中，我们将空间量化为多分辨率的体素网格，以允许在不同的粒度级别添加新的锚点，其中ϵ(m)g = ϵg/4m−1，τ(m)g = τg ∗ 2m−1，（其中m表示量化的级别）。为了进一步调控新锚点的添加，我们对这些候选进行随机消除。这种谨慎的方法有效地抑制了锚点的快速扩展。<br>修剪操作：为了消除无关的锚点，我们累积了与它们关联的神经高斯模型在N个训练迭代中的不透明度值。如果一个锚点未能生成具有满意不透明度水平的神经高斯模型，我们就将其从场景中移除<br><img src="/2024/02/03/scaffold-gs/Scaffold-GS%5C%E5%9B%BE%E7%89%871.png" alt="梯度（从小到大）多分辨率体素（新锚点着色"><br>锚点细化。我们提出了一种基于误差的锚点生长策略，以可靠地在神经高斯人认为重要的位置生长新的锚点。 我们将神经高斯量化为多分辨率体素，并向梯度大于水平阈值的体素添加新的锚点。 我们的策略有效地提高了场景覆盖率，而不会使用过多的点。我们将卡车场景中的初始锚点和精炼锚点可视化。卡车用圆圈标出。注意，精炼点有效地覆盖了周围区域和精细结构，倾向于更完整和详细的场景渲染。<br><img src="/2024/02/03/scaffold-gs/Scaffold-GS%5C%E5%9B%BE%E7%89%872.png" alt="卡车描点细化效果"><br>结果：Scaffold-GS&nbsp;在各种类型场景上的渲染结果。 包括无纹理区域、观测不足、精细细节、视效和多尺度观察等具有挑战性的情况都能得到合理处理。Scaffold-GS&nbsp;对视图相关效应（例如反射、阴影）更鲁棒; 并减轻冗余 3D 高斯引起的伪影（例如漂浮物、结构误差）</p><h3 id="定量比较"><a href="#定量比较" class="headerlink" title="定量比较"></a>定量比较</h3><p><img src="/2024/02/03/scaffold-gs/Scaffold-GS%5C%E5%9B%BE%E7%89%873.png" alt="复杂数据集效果更好、存储空间更小"></p><h3 id="定性比较"><a href="#定性比较" class="headerlink" title="定性比较"></a>定性比较</h3><p><img src="/2024/02/03/scaffold-gs/Scaffold-GS%5C%E5%9B%BE%E7%89%874.png" alt="卡车描点细化效果"><br>在不同数据集上对Scaffold-GS和3D-GS[22]进行定性比较。强调视觉差异的补丁用箭头和绿色和黄色的插图来强调。我们的方法在这些场景中优于3D-GS，在具有挑战性的场景中具有明显的优势，例如薄几何和细尺度细节，无纹理区域，光效果，观察不足。还可以观察到我们的模型在表示不同尺度和观看距离的内容方面具有优势。</p><iframe height="498" width="510" src="Scaffold-GS\compare_apar_t.mp4"></iframe>]]></content>
      
      
      
        <tags>
            
            <tag> 三维重建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mip-splatting</title>
      <link href="/2024/02/03/mip-splatting/"/>
      <url>/2024/02/03/mip-splatting/</url>
      
        <content type="html"><![CDATA[<p>3D高斯展开（3DGS）展示了令人印象深刻的新视图合成结果，达到了高保真度和高效率。 然而，在改变采样率时，例如通过改变焦距或相机距离(3DGS由于使用膨胀而产生膨胀和侵蚀伪影。 当放大或靠近相机时，它会产生侵蚀效果。 这是因为膨胀的 2D 高斯在屏幕空间中变小，使对象结构比实际看起来更薄。 相反，屏幕空间膨胀在缩小或远离场景时会产生膨胀伪影。 在这种情况下，膨胀的 2D 高斯在屏幕空间中变得更大，使对象结构比实际看起来更厚。)。 我们发现，这种现象的根源可以归因于缺乏3D频率约束和使用2D膨胀滤波器。 为了解决这个问题，我们引入了一个 3D 平滑滤波器，该滤波器根据输入视图引起的最大采样频率来约束 3D 高斯基元的大小(将3D表示的频率限制在训练图像确定的最大采样率的一半以下)，从而消除了放大时的高频伪影。 此外，用模拟 2D 盒式滤波器的 2D Mip滤波器代替 2D 膨胀，可有效缓解混叠和膨胀问题。 我们的综合评估，包括单尺度图像训练和多尺度测试等场景，验证了我们方法的有效性。</p><p><img src="/2024/02/03/mip-splatting/01.png"></p><p>3D 高斯展开通过将 3D 对象表示为高斯投影到图像平面上，然后在屏幕空间中进行 2D 膨胀来渲染图像，如 （a） 所示。 该方法的固有收缩偏差导致退化的 3D 高斯超过了采样极限，如 （b） 中的 δ 函数所示，同时由于膨胀操作而呈现类似于 2D。 然而，当改变采样率（通过焦距或相机距离）时，我们观察到强烈的膨胀效应（c）和高频伪像（d）。</p><p>一句话总结：Mip-Splatting为3D GS引入了3D平滑滤波器和2D Mip滤波器，消除了多个伪影。</p><p><img src="/2024/02/03/mip-splatting/02.png" alt="多尺度训练测试"></p><p>Blender数据集上的多尺度训练和多尺度测试，Mip-Splatting在大多数指标上都达到了最优性能，显著优于3DGS和3DGS+EWA(椭圆加权平均滤波器)。</p><p><img src="/2024/02/03/mip-splatting/03.png" alt="单尺度训练测试"></p><p>在Blender数据集上进行单尺度训练和多尺度测试，所有方法都在全分辨率下训练，然后在较小分辨率下进行评估，以模拟放大效果。Mip-Splatting在低分辨率下优于3DGS和3DGS+EWA。</p><p>本文提出了Mip-Splatting，这是对3D高斯Splatting的修改，引入了两个新颖的滤波器，即3D平滑滤波器和2D Mip滤波器，以在任意尺度实现无混叠的渲染。我们的3D平滑滤波器有效地限制了高斯原语的最大频率，以匹配训练图像施加的采样约束，而2D Mip滤波器则近似盒滤波器以模拟物理成像过程。实验结果表明，在相同的尺度/采样率下进行训练和测试时，Mip-Splatting在性能上可与最先进的方法相竞争。重要的是，在分布不一致的情境下，即在不同于训练的采样率下进行测试时，它在超出分布的摄像机姿势和缩放因子的情况下表现显著优越，实现了更好的泛化效果。</p><p>缺陷：该方法使用高斯滤波器作为盒状滤波器的高效近似。然而，这种近似引入了误差，特别是当高斯在屏幕空间中很小时，增大缩小导致了较大的误差，如表2所示。此外，由于每个3D高斯的采样率必须每m = 100次迭代计算一次，训练开销略有增加。</p><p><img src="/2024/02/03/mip-splatting/04.jpg"></p><p>复现过程</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda create -y -n mip-splatting python=3.8</span><br><span class="line">conda activate mip-splatting</span><br><span class="line">pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html</span><br><span class="line">conda install cudatoolkit-dev=11.3 -c conda-forge</span><br><span class="line"># 使用 Conda 包管理器安装 CUDA 工具包的开发版本（cudatoolkit-dev），并指定安装版本为 11.3</span><br></pre></td></tr></tbody></table></figure><p>conda init bash 是指运行 conda init 命令，以初始化您的 Bash shell(是 Bourne Again SHell 的缩写，是一个流行的 Unix/Linux 环境中常见的命令行解释器。) 以便与 Conda 兼容。这个命令的目的是配置您的 shell(命令解释器，允许用户与操作系统进行交互)，以便能够正确使用 conda activate 等 Conda 命令。</p><p><img src="/2024/02/03/mip-splatting/05.png" alt="激活环境"></p><p><img src="/2024/02/03/mip-splatting/06.png"></p><p><img src="/2024/02/03/mip-splatting/07.png"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install submodules/diff-gaussian-rasterization</span><br><span class="line">pip install submodules/simple-knn/</span><br></pre></td></tr></tbody></table></figure><p>清华大学的 PyPI 镜像，可以使用 -i 参数指定镜像地址：pip install -r requirements.txt -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></p><p><img src="/2024/02/03/mip-splatting/08.png"></p><p>对于Blender数据集从<a href="https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1">https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1</a>下载，仅保留ship数据集，运行</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert_blender_data.py --blender_dir nerf_synthetic/ --object_name ship --out_dir multi-scale</span><br></pre></td></tr></tbody></table></figure><p>得到multi-scale,</p><p><img src="/2024/02/03/mip-splatting/09.png"></p><p><img src="/2024/02/03/mip-splatting/010.png"></p><p>运行多尺度评估和测试，单尺度评估和测试，这里仅使用ship数据集，所以scenes需要修改</p><p>single-scale training and single-scale testing on NeRF-synthetic dataset</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/run_nerf_synthetic_stmt.py </span><br></pre></td></tr></tbody></table></figure><p>multi-scale training and multi-scale testing on NeRF-synthetic dataset</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/run_nerf_synthetic_mtmt.py</span><br></pre></td></tr></tbody></table></figure><p>Online viewer: After training, you can fuse the 3D smoothing filter to the Gaussian parameters with</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python create_fused_ply.py -m {model_dir}/{scene} --output_ply fused/{scene}_fused.ply</span><br></pre></td></tr></tbody></table></figure><p>Then use our <a href="https://niujinshuchong.github.io/mip-splatting-demo">online viewer</a> to visualize the trained model.</p><p><img src="/2024/02/03/mip-splatting/011.png" alt="修改ship"></p><p><img src="/2024/02/03/mip-splatting/012.png"></p><p>运行</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python create_fused_ply.py -m benchmark_nerf_synthetic_ours_mtmt/ship --output_ply fused/ship_fused.ply</span><br></pre></td></tr></tbody></table></figure><p>得到fused中.plt,可在<a href="https://niujinshuchong.github.io/mip-splatting-demo/">https://niujinshuchong.github.io/mip-splatting-demo/</a>导入查看。</p><p>对于mip-nerf 360数据集，先将文件夹名修改为360_v2,代码中默认文件名为360_v2,同样只使用flowers数据集，scenes仅保留flowers,之后同上</p><ul><li>single-scale training and single-scale testing on the mip-nerf 360 dataset</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/run_mipnerf360.py </span><br></pre></td></tr></tbody></table></figure><ul><li>single-scale training and multi-scale testing on the mip-nerf 360 dataset</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/run_mipnerf360_stmt.py</span><br></pre></td></tr></tbody></table></figure><p><img src="/2024/02/03/mip-splatting/013.png"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 三维重建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/02/02/hello-world/"/>
      <url>/2024/02/02/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
