<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>RLHF 的三阶段流程</title>
      <link href="/2026/02/23/RLHF%20%E7%9A%84%E4%B8%89%E9%98%B6%E6%AE%B5%E6%B5%81%E7%A8%8B/"/>
      <url>/2026/02/23/RLHF%20%E7%9A%84%E4%B8%89%E9%98%B6%E6%AE%B5%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="RLHF-的三阶段流程"><a href="#RLHF-的三阶段流程" class="headerlink" title="RLHF 的三阶段流程"></a>RLHF 的三阶段流程</h3><p>RLHF（Reinforcement Learning from Human Feedback）通常包含三个主要阶段：</p><p><strong>监督微调（SFT）→ 偏好采样 + 奖励模型学习 → 强化学习优化（RL）</strong></p><p>简单可以理解为：先学”怎么说话”，再学”什么话更好”，最后逼模型多说”好话”</p><h4 id="阶段一：SFT（Supervised-Fine-Tuning）"><a href="#阶段一：SFT（Supervised-Fine-Tuning）" class="headerlink" title="阶段一：SFT（Supervised Fine-Tuning）"></a>阶段一：SFT（Supervised Fine-Tuning）</h4><p>RLHF 通常从对一个预训练语言模型进行监督微调开始，使用高质量的人工数据（如对话、摘要等），得到一个模型 $ \pi_{SFT} $。</p><p><strong>数据形式</strong>：$ (\text{prompt } x, \text{高质量回答 } y) $</p><p><strong>损失函数</strong>：标准的监督学习损失函数</p><script type="math/tex; mode=display">\max_{\theta} \sum_{(x,y) \in D} \log \pi_\theta(y|x)</script><ul><li>$ \pi_\theta $：策略模型的参数化形式</li><li>$ D $：监督微调数据集</li><li>$ x $：输入提示</li><li>$ y $：目标回答</li></ul><p><strong>目标</strong>：让模型学会基本的对话能力和任务完成能力，即”怎么说话”</p><h4 id="阶段二：奖励模型（Reward-Modeling）"><a href="#阶段二：奖励模型（Reward-Modeling）" class="headerlink" title="阶段二：奖励模型（Reward Modeling）"></a>阶段二：奖励模型（Reward Modeling）</h4><p>使用 SFT 模型，对同一个 prompt $ x $ 采样两个回答：</p><script type="math/tex; mode=display">y_1, y_2 \sim \pi_{SFT}(y|x)</script><p>交给人类标注者，让他们选更好的一个：</p><script type="math/tex; mode=display">y_w \succ y_l \mid x</script><ul><li>$ y_w $：winner,preferred（赢家）</li><li>$ y_l $：loser,rejected（输家）</li></ul><p><strong>潜在奖励函数假设</strong>：假设人类偏好是由一个未知的真实奖励函数 $ r^*(x,y) $ 生成的。我们并不知道它，但假设它存在。</p><p><strong>偏好模型</strong>：</p><script type="math/tex; mode=display">p^*(y_1 > y_2 | x) = \frac{\exp(r^*(x, y_1))}{\exp(r^*(x, y_1)) + \exp(r^*(x, y_2))} \tag{1}</script><p><strong>数据集形式</strong>：</p><script type="math/tex; mode=display">D = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N</script><p><strong>损失函数</strong>：</p><script type="math/tex; mode=display">\mathcal{L}_\text{R}(r_\phi, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma (r_\phi(x, y_w) - r_\phi(x, y_l)) \right] \tag{2}</script><hr><h5 id="补充解释"><a href="#补充解释" class="headerlink" title="补充解释"></a>补充解释</h5><p>BT 模型：</p><p>Bradley-Terry 模型是一个用于<strong>成对比较</strong>的概率模型</p><p><strong>核心思想</strong>：</p><ul><li>每个对象 $ i $ 有一个隐藏的”实力值”参数 $ \lambda_i $</li><li>当两个对象 $ i $ 和 $ j $ 比较时，$ i $ 战胜 $ j $ 的概率为：</li></ul><script type="math/tex; mode=display">P(i \succ j) = \frac{\lambda_i}{\lambda_i + \lambda_j}</script><p><strong>在 DPO 中的应用</strong>：</p><ul><li>将奖励函数 $ r(x,y) $ 看作”实力值”</li><li>人类偏好 $ y_w \succ y_l $ 就是”比较结果”</li><li>通过 BT 模型建立奖励与偏好的桥梁</li></ul><p>为什么不用「谁分数大就选谁」？</p><p>如果直接规定：<script type="math/tex">y_1 \succ y_2 \iff r^*(x,y_1) > r^*(x,y_2)</script><br>那问题是：人类判断有噪声，同一个人、同一个问题，不一定每次选同样的。所以我们不建模成确定性规则，而是：概率模型</p><p>把公式 (1) 改写一下：</p><script type="math/tex; mode=display">p^*(y_1 \succ y_2 \mid x) = \frac{1}{1 + e^{-(r^*(x,y_1) - r^*(x,y_2))}}</script><p>这就是熟悉的 Sigmoid 函数：</p><script type="math/tex; mode=display">\sigma(z) = \frac{1}{1 + e^{-z}}</script><p>于是：</p><script type="math/tex; mode=display">p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x,y_1) - r^*(x,y_2))</script><p>因为真实数据里人确实选择了 $ y_w $，所以该事件的概率是上式，对数似然是：</p><script type="math/tex; mode=display">\log p_\theta(y_w \succ y_l \mid x) = \log \sigma(r_\theta(x,y_w) - r_\theta(x,y_l))</script><p>最大化所有样本的对数似然：</p><script type="math/tex; mode=display">\max_\theta \sum_{i=1}^N \log \sigma(r_\theta(x^{(i)}, y_w^{(i)}) - r_\theta(x^{(i)}, y_l^{(i)}))</script><p>意思是：对每一条数据，都算一次”模型认为人类会选赢家的概率的对数”，然后把它们加起来，让这个总和尽可能大。对数把“连乘”变成“连加”,最优解不变(对数是单调递增函数)</p><p>等价地，最小化负对数似然（Likelihood）：</p><script type="math/tex; mode=display">L_R = -\mathbb{E}_{(x,y_w,y_l) \sim D} [\log \sigma(r_\theta(x,y_w) - r_\theta(x,y_l))]</script><p>这正是公式 (2)。意思是：对数据集里的样本取平均，然后对”对数概率”取负号，让这个值尽可能小。</p><p>对一个有限数据集 $ D = {z_1, …, z_N} $，如果你均匀随机从中抽一个样本，那么：</p><script type="math/tex; mode=display">\mathbb{E}_{z \sim D} [f(z)] = \frac{1}{N} \sum_{i=1}^N f(z_i)</script><p>👉 期望 = 平均值</p><p>令 $ f(x,y<em>w,y_l) = \log \sigma(r</em>\theta(x,y<em>w) - r</em>\theta(x,y_l)) $ 那么：</p><script type="math/tex; mode=display">\mathbb{E}_{(x,y_w,y_l) \sim D} [f] = \frac{1}{N} \sum_{i=1}^N \log \sigma(r_\theta(x^{(i)}, y_w^{(i)}) - r_\theta(x^{(i)}, y_l^{(i)}))</script><p>因为 $ \sum<em>{i=1}^N f_i $ 和 $ \frac{1}{N} \sum</em>{i=1}^N f<em>i $ 只差一个常数 $ \frac{1}{N} $。而在优化中：乘以一个正的常数，不会改变最优解的位置。也就是说：$$ \arg\max</em>\theta \sum<em>{i=1}^N f_i = \arg\max</em>\theta \frac{1}{N} \sum_{i=1}^N f_i $$</p><p>原目标：$ \max_\theta \mathbb{E}_D [\log \sigma(\cdots)] $</p><p>等价于：$ \min_\theta -\mathbb{E}_D [\log \sigma(\cdots)] $</p><p>于是定义：$ L_R = -\mathbb{E}_D [\log \sigma(\cdots)] $</p><p><strong>为什么机器学习里”总是最小化”？</strong></p><p>因为梯度下降（Gradient Descent）默认是 minimize loss，所以大家习惯：把”想最大化的目标”，写成”要最小化的损失”。</p><hr><h4 id="阶段三：RL-微调"><a href="#阶段三：RL-微调" class="headerlink" title="阶段三：RL 微调"></a>阶段三：RL 微调</h4><p><strong>RL 目标函数（核心）</strong>：</p><script type="math/tex; mode=display">\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y|x)} \left[ r_\phi(x, y) - \beta D_{\text{KL}} \left[ \pi_\theta(y|x) \middle\| \pi_{\text{ref}}(y|x) \right] \right] \tag{3}</script><hr><h5 id="补充解释-1"><a href="#补充解释-1" class="headerlink" title="补充解释"></a>补充解释</h5><p>一句话解释：👉 让模型生成 奖励高的回答 👉 但不能偏离原始 SFT 模型太远</p><p>$ \pi<em>{ref} $（即初始 SFT 模型 $ \pi</em>{SFT} $）实践中，语言模型策略 $ \pi<em>\theta $ 也被初始化为 $ \pi</em>{SFT} $。</p><ul><li><strong>初始状态</strong>：$ \pi<em>\theta = \pi</em>{ref} = \pi_{SFT} $</li><li><strong>训练目标</strong>：在保持与原始 SFT 模型接近的前提下，优化奖励</li><li><strong>KL 约束作用</strong>：确保模型不会偏离初始能力太远</li></ul><p><strong>KL 约束：</strong></p><p>KL 散度（Kullback-Leibler 散度）衡量两个概率分布之间的差异：</p><script type="math/tex; mode=display">D_{\text{KL}}[P \| Q] = \sum_x P(x) \log \frac{P(x)}{Q(x)}</script><script type="math/tex; mode=display">D_{\text{KL}}[\pi_\theta(y|x) \| \pi_{SFT}(y|x)] = \mathbb{E}_{y \sim \pi_\theta} \left[ \log \frac{\pi_\theta(y|x)}{\pi_{SFT}(y|x)} \right]</script><p>这表示：</p><ul><li>如果 $ \pi<em>\theta $ 和 $ \pi</em>{SFT} $ 相同：KL = 0</li><li>如果 $ \pi<em>\theta $ 偏离 $ \pi</em>{SFT} $：KL &gt; 0</li><li>目标函数中的 $ -\beta KL $ 惩罚偏离行为</li></ul><p><strong>KL 散度推导过程详解</strong></p><p><strong>KL 散度的通用定义（离散型）</strong>：</p><script type="math/tex; mode=display">D_{\text{KL}}[P \| Q] = \sum_i P(i) \log \frac{P(i)}{Q(i)}</script><p><strong>期望的定义</strong>：<br>对于随机变量 $ Y \sim P $，函数 $ f(Y) $ 的期望定义为：</p><script type="math/tex; mode=display">\mathbb{E}_{Y \sim P}[f(Y)] = \sum_{y \in \mathcal{Y}} P(y) f(y)</script><p><strong>进行变量映射</strong></p><p>在 RLHF 的语境下，我们要计算的是两个模型策略（概率分布）之间的差异：</p><ul><li>$ P $<strong> (当前分布)</strong>：$ \pi_\theta(y|x) $ —— 模型当前正在学习的策略</li><li>$ Q $<strong> (参考分布)</strong>：$ \pi_{SFT}(y|x) $ —— 原始的 SFT 策略</li><li><strong>样本空间</strong>：$ y $（模型生成的所有可能的回答序列）</li></ul><p>将这些代入通用公式：</p><script type="math/tex; mode=display">D_{\text{KL}}[\pi_\theta \| \pi_{SFT}] = \sum_y \pi_\theta(y|x) \log \frac{\pi_\theta(y|x)}{\pi_{SFT}(y|x)}</script><p><strong>转化为期望形式</strong></p><ul><li>外层的 $ \sum<em>y \pi</em>\theta(y|x)(\cdots) $ 表示我们在按照 $ \pi_\theta(y|x) $ 的概率分布对后面的项进行加权平均</li><li>括号里的项 $ \log \frac{\pi<em>\theta(y|x)}{\pi</em>{SFT}(y|x)} $ 就是要计算期望的对象</li></ul><p>因此，根据期望的定义 $ \sum P \cdot f = \mathbb{E}_P[f] $，可以直接写成：</p><script type="math/tex; mode=display">D_{\text{KL}}[\pi_\theta \| \pi_{SFT}] = \mathbb{E}_{y \sim \pi_\theta(y|x)} \left[ \log \frac{\pi_\theta(y|x)}{\pi_{SFT}(y|x)} \right]</script><p><strong>为什么实践中要写成期望形式？</strong></p><p>在深度学习和强化学习中，写成期望形式有重大的工程意义：</p><p><strong>无法全量求和</strong>：<br>对于语言模型，可能的回答 $ y $ 的组合是天文数字（由词表大小和序列长度决定），我们根本无法遍历所有的 $ y $ 来计算那个 $ \sum $ 符号。</p><p><strong>蒙特卡洛采样（Monte Carlo Sampling）</strong>：<br>期望形式告诉我们，虽然不能遍历所有结果，但可以通过采样来近似：让模型 $ \pi<em>\theta $ 生成（Sample）一批句子 $ y $,计算这些句子的 $ \log \pi</em>\theta(y|x) - \log \pi_{SFT}(y|x) $,取这些值的平均值，就是对 KL 散度的无偏估计</p><p><strong>对齐目标函数</strong>：<br>RL 的目标函数本身就是最大化期望奖励 $ \mathbb{E}<em>{y \sim \pi</em>\theta}[r(x,y)] $。将 KL 约束也写成期望形式，就可以合并到一个大括号里：</p><p>$ \max<em>{\pi</em>\theta} \mathbb{E}<em>{y \sim \pi</em>\theta} \left[ r(x,y) - \beta \log \frac{\pi<em>\theta(y|x)}{\pi</em>{SFT}(y|x)} \right] $</p><p>这样，模型在每一轮训练采样时，就可以同时优化奖励并计算 KL 惩罚。</p><p><strong>为什么这个目标函数不可导？</strong></p><p>语言模型是<strong>离散生成</strong>的：</p><ul><li>输出是 token 序列 $ y = (y_1, y_2, \ldots) $</li><li>采样 / argmax 都是离散操作</li></ul><p>无法像普通监督学习那样，对 $ \mathbb{E}<em>{y \sim \pi</em>\theta(y|x)}[r(x,y)] $ 直接对 $ \theta $ 求梯度。</p><p>👉 所以不能用反向传播直接优化奖励<br>👉 必须用强化学习（policy gradient）</p><p>这就是为什么要用 REINFORCE / PPO。</p><p><strong>KL 约束是怎么变成”奖励”的？</strong></p><p><strong>关键一步是：把公式 (3) KL 项写成 log-prob 的形式</strong></p><p>对离散策略：</p><p>$ D<em>{\text{KL}}(\pi</em>\theta | \pi<em>{ref}) = \mathbb{E}</em>{y \sim \pi<em>\theta} \left[ \log \pi</em>\theta(y|x) - \log \pi_{ref}(y|x) \right] $</p><p><strong>代回目标函数</strong>：</p><p>$ \mathbb{E}<em>{y \sim \pi</em>\theta} \left[ r<em>\phi(x, y) - \beta \left( \log \pi</em>\theta(y|x) - \log \pi_{ref}(y|x) \right) \right] $</p><p><strong>于是可以定义一个新的 reward</strong>：</p><p>$ r(x, y) = r<em>\phi(x, y) - \beta \left( \log \pi</em>\theta(y|x) - \log \pi_{ref}(y|x) \right) $</p><p>即论文中的：</p><p>$ r(x, y) = r<em>\phi(x, y) - \left( \log \pi</em>\theta(y|x) - \log \pi_{ref}(y|x) \right) $</p><p>（论文里通常把 $ \beta $ 省略或吸收到系数里）</p><p><strong>转换后的简化目标函数</strong></p><p>通过这个变换，原来的约束优化问题变成了无约束的奖励最大化问题：</p><p>$ \max<em>{\pi</em>\theta} \mathbb{E}<em>{x \sim \mathcal{D}, y \sim \pi</em>\theta(y|x)} \left[ r(x, y) \right] $</p><p>其中 $ r(x, y) $ 已经包含了 KL 惩罚项。</p><hr><h2 id="直接偏好优化（Direct-Preference-Optimization）"><a href="#直接偏好优化（Direct-Preference-Optimization）" class="headerlink" title="直接偏好优化（Direct Preference Optimization）"></a>直接偏好优化（Direct Preference Optimization）</h2>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>力扣 88：合并两个有序数组</title>
      <link href="/2026/02/23/%E5%8A%9B%E6%89%A3/"/>
      <url>/2026/02/23/%E5%8A%9B%E6%89%A3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定两个按非递减顺序排列的整数数组 <code>nums1</code> 和 <code>nums2</code>，分别有长度 <code>m</code>、<code>n</code>。请将 <code>nums2</code> <strong>原地</strong> 合并到 <code>nums1</code>，使结果仍保持非递减顺序。<code>nums1</code> 的总长度为 <code>m + n</code>，末尾的 <code>n</code> 个位置初始化为 <code>0</code> 作为缓冲区。</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>与其先拼接再整体排序，不如利用“两数组已排序”这一特性，从尾部开始归并：</p><ol><li>设 <code>p1 = m - 1</code>、<code>p2 = n - 1</code> 分别指向两个数组的最后一个有效元素，<code>tail = m + n - 1</code> 指向合并后数组的尾部；</li><li>每轮比较 <code>nums1[p1]</code> 和 <code>nums2[p2]</code>，把较大的放入 <code>nums1[tail]</code>，然后移动对应指针以及 <code>tail</code>；</li><li>若 <code>nums2</code> 还有剩余元素（<code>p2 &gt;= 0</code>），继续写入；如果只剩 <code>nums1</code> 元素，无需处理，因为它们已经在正确位置。</li></ol><p>双指针从后往前填充可以避免移动大量元素，时间复杂度为 $\mathcal{O}(m+n)$，空间复杂度为 $\mathcal{O}(1)$。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">merge</span>(<span class="params">self, nums1: <span class="type">List</span>[<span class="built_in">int</span>], m: <span class="built_in">int</span>, nums2: <span class="type">List</span>[<span class="built_in">int</span>], n: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        p1 = m - <span class="number">1</span></span><br><span class="line">        p2 = n - <span class="number">1</span></span><br><span class="line">        tail = m + n - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> p2 &gt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> p1 &gt;= <span class="number">0</span> <span class="keyword">and</span> nums1[p1] &gt; nums2[p2]:</span><br><span class="line">                nums1[tail] = nums1[p1]</span><br><span class="line">                p1 -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                nums1[tail] = nums2[p2]</span><br><span class="line">                p2 -= <span class="number">1</span></span><br><span class="line">            tail -= <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><strong>时间复杂度</strong>：$\mathcal{O}(m+n)$，每个元素只移动一次；</li><li><strong>空间复杂度</strong>：$\mathcal{O}(1)$，原地合并未使用额外数组。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 力扣 </tag>
            
            <tag> 数组 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
